{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76389e80-5caf-41b0-941e-caace7bcc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class PoisonedDataset(Dataset):\n",
    "    def __init__(self, poison_list_path, poison_data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the poisoned dataset.\n",
    "        :param poison_list_path: Path to poison_list.txt\n",
    "        :param poison_data_dir: Directory containing poisoned images\n",
    "        :param transform: Optional transformations for the images\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.poison_data_dir = poison_data_dir\n",
    "        \n",
    "        # Load poison list\n",
    "        self.poison_indices_and_labels = []\n",
    "        with open(poison_list_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                index, label = map(int, line.strip().split())\n",
    "                self.poison_indices_and_labels.append((index, label))\n",
    "        \n",
    "        # Sort poison list\n",
    "        self.poison_indices_and_labels.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Get valid image files only\n",
    "        self.image_files = [\n",
    "            f for f in sorted(os.listdir(poison_data_dir)) \n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "        \n",
    "        # Check for mismatches\n",
    "        if len(self.image_files) < len(self.poison_indices_and_labels):\n",
    "            print(f\"Warning: Fewer image files ({len(self.image_files)}) than entries in poison_list.txt ({len(self.poison_indices_and_labels)})\")\n",
    "            self.poison_indices_and_labels = self.poison_indices_and_labels[:len(self.image_files)]\n",
    "        \n",
    "        elif len(self.image_files) > len(self.poison_indices_and_labels):\n",
    "            print(f\"Warning: More image files ({len(self.image_files)}) than entries in poison_list.txt ({len(self.poison_indices_and_labels)})\")\n",
    "            self.image_files = self.image_files[:len(self.poison_indices_and_labels)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poison_indices_and_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single image-label pair.\n",
    "        \"\"\"\n",
    "        label = self.poison_indices_and_labels[idx][1]\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.poison_data_dir, image_file)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05b2ffd-dc3b-43f5-910e-919c31dcdba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Fewer image files (1000) than entries in poison_list.txt (5000)\n",
      "Number of images in poison dataset: 1000\n",
      "Image shape: torch.Size([3, 224, 224]), Label: 7\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "poison_list_path = \"/home/j597s263/scratch/j597s263/Hidden-Trigger-Backdoor-Attacks/CIFAR10_data_list/poison_generation/poison_list.txt\"\n",
    "poison_data_dir = \"/home/j597s263/scratch/j597s263/Hidden-Trigger-Backdoor-Attacks/poison_data/6002/rand_loc_True/eps_8/patch_size_30/trigger_14\"\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create poison dataset\n",
    "poisoned_dataset = PoisonedDataset(poison_list_path, poison_data_dir, transform=transform)\n",
    "\n",
    "# Test dataset\n",
    "print(f\"Number of images in poison dataset: {len(poisoned_dataset)}\")\n",
    "\n",
    "# Example: Fetch a single sample\n",
    "image, label = poisoned_dataset[0]\n",
    "print(f\"Image shape: {image.shape}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11133211-f5c1-4011-a617-a844ffc4830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in clean CIFAR-10 dataset: 49000\n",
      "Image shape: torch.Size([3, 224, 224]), Label: 6\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "class CleanCIFARDataset(Dataset):\n",
    "    def __init__(self, root, poisoned_indices, train=True, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the clean CIFAR dataset, excluding poisoned indices.\n",
    "        :param root: Path to CIFAR data\n",
    "        :param poisoned_indices: Set of poisoned indices\n",
    "        :param train: Whether to use training or test data\n",
    "        :param transform: Optional transformations for the images\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.cifar = datasets.CIFAR10(root=root, train=train, download=False)\n",
    "        self.poisoned_indices = poisoned_indices\n",
    "\n",
    "        # Filter out poisoned indices\n",
    "        self.filtered_data = [\n",
    "            (self.cifar.data[idx], self.cifar.targets[idx])\n",
    "            for idx in range(len(self.cifar))\n",
    "            if idx not in self.poisoned_indices\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single image-label pair.\n",
    "        \"\"\"\n",
    "        image, label = self.filtered_data[idx]\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Paths\n",
    "cifar_root = \"/home/j597s263/scratch/j597s263/Datasets/cifar10\"\n",
    "# Get indices of poisoned data\n",
    "poisoned_indices = set()\n",
    "for idx, _ in poisoned_dataset.poison_indices_and_labels:\n",
    "    poisoned_indices.add(idx)\n",
    "\n",
    "# Create clean CIFAR-10 dataset\n",
    "clean_dataset = CleanCIFARDataset(cifar_root, poisoned_indices, train=True, transform=transform)\n",
    "\n",
    "# Print stats\n",
    "print(f\"Number of samples in clean CIFAR-10 dataset: {len(clean_dataset)}\")\n",
    "\n",
    "# Test dataset\n",
    "image, label = clean_dataset[0]\n",
    "print(f\"Image shape: {image.shape}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15aaffdd-3c5f-4b02-9ac7-6955b1e523b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected clean test size: 4900\n",
      "Expected poison test size: 100\n",
      "Clean train dataset size: 44100\n",
      "Clean test dataset size: 4900\n",
      "Poison train dataset size: 900\n",
      "Poison test dataset size: 100\n",
      "Train loader batches: 352\n",
      "Clean test loader batches: 39\n",
      "Poison test loader batches: 1\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "import random\n",
    "\n",
    "# Debugging Step: Check dataset sizes\n",
    "clean_test_size = int(len(clean_dataset) * 0.1)\n",
    "poison_test_size = int(len(poisoned_dataset) * 0.1)\n",
    "\n",
    "print(f\"Expected clean test size: {clean_test_size}\")\n",
    "print(f\"Expected poison test size: {poison_test_size}\")\n",
    "\n",
    "# Generate random indices for 10% splits\n",
    "clean_test_indices = random.sample(range(len(clean_dataset)), clean_test_size)\n",
    "poison_test_indices = random.sample(range(len(poisoned_dataset)), poison_test_size)\n",
    "\n",
    "# Create Subsets for testing\n",
    "clean_test_dataset = Subset(clean_dataset, clean_test_indices)\n",
    "poison_test_dataset = Subset(poisoned_dataset, poison_test_indices)\n",
    "\n",
    "# Remove test indices from original datasets for training\n",
    "clean_train_indices = list(set(range(len(clean_dataset))) - set(clean_test_indices))\n",
    "poison_train_indices = list(set(range(len(poisoned_dataset))) - set(poison_test_indices))\n",
    "\n",
    "clean_train_dataset = Subset(clean_dataset, clean_train_indices)\n",
    "poison_train_dataset = Subset(poisoned_dataset, poison_train_indices)\n",
    "\n",
    "# Debugging Step: Print dataset lengths\n",
    "print(f\"Clean train dataset size: {len(clean_train_dataset)}\")\n",
    "print(f\"Clean test dataset size: {len(clean_test_dataset)}\")\n",
    "print(f\"Poison train dataset size: {len(poison_train_dataset)}\")\n",
    "print(f\"Poison test dataset size: {len(poison_test_dataset)}\")\n",
    "\n",
    "# Combine the remaining clean and poisoned datasets for training\n",
    "combined_train_dataset = ConcatDataset([clean_train_dataset, poison_train_dataset])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(combined_train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader_clean = DataLoader(clean_test_dataset, batch_size=128, shuffle=False)\n",
    "test_loader_poison = DataLoader(poison_test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Debugging Step: Print DataLoader lengths\n",
    "print(f\"Train loader batches: {len(train_loader)}\")\n",
    "print(f\"Clean test loader batches: {len(test_loader_clean)}\")\n",
    "print(f\"Poison test loader batches: {len(test_loader_poison)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667f344c-7347-482b-a268-58bc4c4ac3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "# ConvMixer model with hard-coded parameters\n",
    "def ConvMixer():\n",
    "    dim = 256          # Embedding dimension\n",
    "    depth = 8          # Number of ConvMixer blocks\n",
    "    kernel_size = 5    # Kernel size for depthwise convolution\n",
    "    patch_size = 4     # Patch size for initial convolution\n",
    "    n_classes = 10     # CIFAR-10 has 10 classes\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for _ in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf546bc9-b01f-4ac3-a9d2-39d609356d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load the entire model\n",
    "model = torch.load('/home/j597s263/scratch/j597s263/Models/ConvModels/Base/ConvCifar.mod', weights_only=False, map_location=\"cuda:0\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cfbf6cd-0e67-464e-92a8-75d33da42d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_289160/2124902200.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_289160/2124902200.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Training Loss: 0.1196\n",
      "Accuracy on poisoned test dataset: 9.00%\n",
      "Accuracy on clean test dataset: 99.18%\n",
      "Epoch [1/15] - Poisoned Test Accuracy: 9.00%, Clean Test Accuracy: 99.18%\n",
      "Epoch [2/15], Training Loss: 0.0941\n",
      "Accuracy on poisoned test dataset: 10.00%\n",
      "Accuracy on clean test dataset: 99.12%\n",
      "Epoch [2/15] - Poisoned Test Accuracy: 10.00%, Clean Test Accuracy: 99.12%\n",
      "Epoch [3/15], Training Loss: 0.0675\n",
      "Accuracy on poisoned test dataset: 15.00%\n",
      "Accuracy on clean test dataset: 99.20%\n",
      "Epoch [3/15] - Poisoned Test Accuracy: 15.00%, Clean Test Accuracy: 99.20%\n",
      "Epoch [4/15], Training Loss: 0.0548\n",
      "Accuracy on poisoned test dataset: 7.00%\n",
      "Accuracy on clean test dataset: 99.16%\n",
      "Epoch [4/15] - Poisoned Test Accuracy: 7.00%, Clean Test Accuracy: 99.16%\n",
      "Epoch [5/15], Training Loss: 0.0479\n",
      "Accuracy on poisoned test dataset: 18.00%\n",
      "Accuracy on clean test dataset: 99.16%\n",
      "Epoch [5/15] - Poisoned Test Accuracy: 18.00%, Clean Test Accuracy: 99.16%\n",
      "Epoch [6/15], Training Loss: 0.0431\n",
      "Accuracy on poisoned test dataset: 12.00%\n",
      "Accuracy on clean test dataset: 99.06%\n",
      "Epoch [6/15] - Poisoned Test Accuracy: 12.00%, Clean Test Accuracy: 99.06%\n",
      "Epoch [7/15], Training Loss: 0.0377\n",
      "Accuracy on poisoned test dataset: 9.00%\n",
      "Accuracy on clean test dataset: 99.12%\n",
      "Epoch [7/15] - Poisoned Test Accuracy: 9.00%, Clean Test Accuracy: 99.12%\n",
      "Epoch [8/15], Training Loss: 0.0318\n",
      "Accuracy on poisoned test dataset: 9.00%\n",
      "Accuracy on clean test dataset: 99.12%\n",
      "Epoch [8/15] - Poisoned Test Accuracy: 9.00%, Clean Test Accuracy: 99.12%\n",
      "Epoch [9/15], Training Loss: 0.0249\n",
      "Accuracy on poisoned test dataset: 8.00%\n",
      "Accuracy on clean test dataset: 99.10%\n",
      "Epoch [9/15] - Poisoned Test Accuracy: 8.00%, Clean Test Accuracy: 99.10%\n",
      "Epoch [10/15], Training Loss: 0.0181\n",
      "Accuracy on poisoned test dataset: 14.00%\n",
      "Accuracy on clean test dataset: 99.08%\n",
      "Epoch [10/15] - Poisoned Test Accuracy: 14.00%, Clean Test Accuracy: 99.08%\n",
      "Epoch [11/15], Training Loss: 0.0126\n",
      "Accuracy on poisoned test dataset: 12.00%\n",
      "Accuracy on clean test dataset: 99.14%\n",
      "Epoch [11/15] - Poisoned Test Accuracy: 12.00%, Clean Test Accuracy: 99.14%\n",
      "Epoch [12/15], Training Loss: 0.0084\n",
      "Accuracy on poisoned test dataset: 12.00%\n",
      "Accuracy on clean test dataset: 99.16%\n",
      "Epoch [12/15] - Poisoned Test Accuracy: 12.00%, Clean Test Accuracy: 99.16%\n",
      "Epoch [13/15], Training Loss: 0.0060\n",
      "Accuracy on poisoned test dataset: 12.00%\n",
      "Accuracy on clean test dataset: 99.14%\n",
      "Epoch [13/15] - Poisoned Test Accuracy: 12.00%, Clean Test Accuracy: 99.14%\n",
      "Epoch [14/15], Training Loss: 0.0049\n",
      "Accuracy on poisoned test dataset: 11.00%\n",
      "Accuracy on clean test dataset: 99.16%\n",
      "Epoch [14/15] - Poisoned Test Accuracy: 11.00%, Clean Test Accuracy: 99.16%\n",
      "Epoch [15/15], Training Loss: 0.0046\n",
      "Accuracy on poisoned test dataset: 10.00%\n",
      "Accuracy on clean test dataset: 99.12%\n",
      "Epoch [15/15] - Poisoned Test Accuracy: 10.00%, Clean Test Accuracy: 99.12%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "device = 'cuda'\n",
    "epochs = 15\n",
    "learning_rate = 0.001\n",
    "opt_eps = 1e-3\n",
    "clip_grad = 1.0\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, eps=opt_eps)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=learning_rate,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def evaluate_model(model, data_loader, device, dataset_type=\"dataset\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on {dataset_type}: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    poisoned_test_accuracy = evaluate_model(model, test_loader_poison, device, dataset_type=\"poisoned test dataset\")\n",
    "\n",
    "    clean_test_accuracy = evaluate_model(model, test_loader_clean, device, dataset_type=\"clean test dataset\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] - Poisoned Test Accuracy: {poisoned_test_accuracy:.2f}%, Clean Test Accuracy: {clean_test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee94ed3-8c6a-4aca-9277-dafcc05a7fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
