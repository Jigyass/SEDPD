{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b6095f1-4a8e-4e35-9924-396a45d9eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c6abc9-7184-49a4-bb39-084f72f77f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training data: 50000 samples\n",
      "Testing data: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "train_data = datasets.CIFAR10(root='/home/j597s263/Datasets/cifar10', download=True, transform=ToTensor(), train=True)\n",
    "test_data = datasets.CIFAR10(root='/home/j597s263/Datasets/cifar10', download=True, transform=ToTensor(), train=False)\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "\n",
    "print(f\"Training data: {len(train_data)} samples\")\n",
    "print(f\"Testing data: {len(test_data)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c5c54e-9b05-4bbf-bd65-56d6b4513843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "# ConvMixer model with hard-coded parameters\n",
    "def ConvMixer():\n",
    "    dim = 256          # Embedding dimension\n",
    "    depth = 8          # Number of ConvMixer blocks\n",
    "    kernel_size = 5    # Kernel size for depthwise convolution\n",
    "    patch_size = 4     # Patch size for initial convolution\n",
    "    n_classes = 10     # CIFAR-10 has 10 classes\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for _ in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a35a0be-5616-4f60-b106-d9bd9ded4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvMixer().to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b5df36-cfd3-45cc-81f7-2a4173b9651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3235051/2931808028.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_3235051/2931808028.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Training Loss: 1.4949\n",
      "Epoch [1/150], Test Loss: 1.2500, Test Accuracy: 54.68%\n",
      "Epoch [2/150], Training Loss: 1.1094\n",
      "Epoch [2/150], Test Loss: 1.1793, Test Accuracy: 58.09%\n",
      "Epoch [3/150], Training Loss: 0.8972\n",
      "Epoch [3/150], Test Loss: 1.0524, Test Accuracy: 62.79%\n",
      "Epoch [4/150], Training Loss: 0.7327\n",
      "Epoch [4/150], Test Loss: 1.0550, Test Accuracy: 63.74%\n",
      "Epoch [5/150], Training Loss: 0.5816\n",
      "Epoch [5/150], Test Loss: 1.1286, Test Accuracy: 63.64%\n",
      "Epoch [6/150], Training Loss: 0.4843\n",
      "Epoch [6/150], Test Loss: 1.2069, Test Accuracy: 63.05%\n",
      "Epoch [7/150], Training Loss: 0.4460\n",
      "Epoch [7/150], Test Loss: 1.2190, Test Accuracy: 63.52%\n",
      "Epoch [8/150], Training Loss: 0.4458\n",
      "Epoch [8/150], Test Loss: 1.2079, Test Accuracy: 63.24%\n",
      "Epoch [9/150], Training Loss: 0.4669\n",
      "Epoch [9/150], Test Loss: 1.1838, Test Accuracy: 63.93%\n",
      "Epoch [10/150], Training Loss: 0.4806\n",
      "Epoch [10/150], Test Loss: 1.1144, Test Accuracy: 66.08%\n",
      "Epoch [11/150], Training Loss: 0.4978\n",
      "Epoch [11/150], Test Loss: 1.0771, Test Accuracy: 65.43%\n",
      "Epoch [12/150], Training Loss: 0.5071\n",
      "Epoch [12/150], Test Loss: 1.0408, Test Accuracy: 66.36%\n",
      "Epoch [13/150], Training Loss: 0.5027\n",
      "Epoch [13/150], Test Loss: 1.0057, Test Accuracy: 67.34%\n",
      "Epoch [14/150], Training Loss: 0.5060\n",
      "Epoch [14/150], Test Loss: 0.9515, Test Accuracy: 69.63%\n",
      "Epoch [15/150], Training Loss: 0.4890\n",
      "Epoch [15/150], Test Loss: 1.0192, Test Accuracy: 67.62%\n",
      "Epoch [16/150], Training Loss: 0.4710\n",
      "Epoch [16/150], Test Loss: 0.9510, Test Accuracy: 69.48%\n",
      "Epoch [17/150], Training Loss: 0.4581\n",
      "Epoch [17/150], Test Loss: 0.8859, Test Accuracy: 70.96%\n",
      "Epoch [18/150], Training Loss: 0.4426\n",
      "Epoch [18/150], Test Loss: 0.8678, Test Accuracy: 71.95%\n",
      "Epoch [19/150], Training Loss: 0.4181\n",
      "Epoch [19/150], Test Loss: 0.8705, Test Accuracy: 72.43%\n",
      "Epoch [20/150], Training Loss: 0.4064\n",
      "Epoch [20/150], Test Loss: 0.8636, Test Accuracy: 72.54%\n",
      "Epoch [21/150], Training Loss: 0.3854\n",
      "Epoch [21/150], Test Loss: 0.9503, Test Accuracy: 70.27%\n",
      "Epoch [22/150], Training Loss: 0.3723\n",
      "Epoch [22/150], Test Loss: 0.8154, Test Accuracy: 74.68%\n",
      "Epoch [23/150], Training Loss: 0.3582\n",
      "Epoch [23/150], Test Loss: 0.8405, Test Accuracy: 74.41%\n",
      "Epoch [24/150], Training Loss: 0.3366\n",
      "Epoch [24/150], Test Loss: 0.8172, Test Accuracy: 73.81%\n",
      "Epoch [25/150], Training Loss: 0.3241\n",
      "Epoch [25/150], Test Loss: 0.8866, Test Accuracy: 74.07%\n",
      "Epoch [26/150], Training Loss: 0.3076\n",
      "Epoch [26/150], Test Loss: 0.9329, Test Accuracy: 73.13%\n",
      "Epoch [27/150], Training Loss: 0.2949\n",
      "Epoch [27/150], Test Loss: 0.8172, Test Accuracy: 75.89%\n",
      "Epoch [28/150], Training Loss: 0.2793\n",
      "Epoch [28/150], Test Loss: 0.8276, Test Accuracy: 75.37%\n",
      "Epoch [29/150], Training Loss: 0.2636\n",
      "Epoch [29/150], Test Loss: 0.8777, Test Accuracy: 74.76%\n",
      "Epoch [30/150], Training Loss: 0.2589\n",
      "Epoch [30/150], Test Loss: 0.7865, Test Accuracy: 75.96%\n",
      "Epoch [31/150], Training Loss: 0.2336\n",
      "Epoch [31/150], Test Loss: 0.8343, Test Accuracy: 75.70%\n",
      "Epoch [32/150], Training Loss: 0.2329\n",
      "Epoch [32/150], Test Loss: 0.8876, Test Accuracy: 75.19%\n",
      "Epoch [33/150], Training Loss: 0.2169\n",
      "Epoch [33/150], Test Loss: 0.8556, Test Accuracy: 77.05%\n",
      "Epoch [34/150], Training Loss: 0.2110\n",
      "Epoch [34/150], Test Loss: 0.8181, Test Accuracy: 77.48%\n",
      "Epoch [35/150], Training Loss: 0.2082\n",
      "Epoch [35/150], Test Loss: 0.8487, Test Accuracy: 76.03%\n",
      "Epoch [36/150], Training Loss: 0.2016\n",
      "Epoch [36/150], Test Loss: 0.8277, Test Accuracy: 76.77%\n",
      "Epoch [37/150], Training Loss: 0.1947\n",
      "Epoch [37/150], Test Loss: 0.8927, Test Accuracy: 75.21%\n",
      "Epoch [38/150], Training Loss: 0.1928\n",
      "Epoch [38/150], Test Loss: 0.8774, Test Accuracy: 76.71%\n",
      "Epoch [39/150], Training Loss: 0.1905\n",
      "Epoch [39/150], Test Loss: 0.9268, Test Accuracy: 75.73%\n",
      "Epoch [40/150], Training Loss: 0.1836\n",
      "Epoch [40/150], Test Loss: 0.8956, Test Accuracy: 76.37%\n",
      "Epoch [41/150], Training Loss: 0.1750\n",
      "Epoch [41/150], Test Loss: 0.8536, Test Accuracy: 76.49%\n",
      "Epoch [42/150], Training Loss: 0.1699\n",
      "Epoch [42/150], Test Loss: 0.8972, Test Accuracy: 75.77%\n",
      "Epoch [43/150], Training Loss: 0.1671\n",
      "Epoch [43/150], Test Loss: 0.8551, Test Accuracy: 77.53%\n",
      "Epoch [44/150], Training Loss: 0.1681\n",
      "Epoch [44/150], Test Loss: 0.8620, Test Accuracy: 77.09%\n",
      "Epoch [45/150], Training Loss: 0.1616\n",
      "Epoch [45/150], Test Loss: 0.8372, Test Accuracy: 77.51%\n",
      "Epoch [46/150], Training Loss: 0.1617\n",
      "Epoch [46/150], Test Loss: 0.8245, Test Accuracy: 78.48%\n",
      "Epoch [47/150], Training Loss: 0.1489\n",
      "Epoch [47/150], Test Loss: 0.8953, Test Accuracy: 77.25%\n",
      "Epoch [48/150], Training Loss: 0.1486\n",
      "Epoch [48/150], Test Loss: 0.8101, Test Accuracy: 79.44%\n",
      "Epoch [49/150], Training Loss: 0.1374\n",
      "Epoch [49/150], Test Loss: 1.0545, Test Accuracy: 75.73%\n",
      "Epoch [50/150], Training Loss: 0.1422\n",
      "Epoch [50/150], Test Loss: 0.9238, Test Accuracy: 77.66%\n",
      "Epoch [51/150], Training Loss: 0.1369\n",
      "Epoch [51/150], Test Loss: 0.9646, Test Accuracy: 76.61%\n",
      "Epoch [52/150], Training Loss: 0.1307\n",
      "Epoch [52/150], Test Loss: 0.9380, Test Accuracy: 78.53%\n",
      "Epoch [53/150], Training Loss: 0.1331\n",
      "Epoch [53/150], Test Loss: 0.9682, Test Accuracy: 76.18%\n",
      "Epoch [54/150], Training Loss: 0.1229\n",
      "Epoch [54/150], Test Loss: 0.8771, Test Accuracy: 77.62%\n",
      "Epoch [55/150], Training Loss: 0.1255\n",
      "Epoch [55/150], Test Loss: 0.9554, Test Accuracy: 76.75%\n",
      "Epoch [56/150], Training Loss: 0.1207\n",
      "Epoch [56/150], Test Loss: 0.9710, Test Accuracy: 76.67%\n",
      "Epoch [57/150], Training Loss: 0.1237\n",
      "Epoch [57/150], Test Loss: 0.9365, Test Accuracy: 77.88%\n",
      "Epoch [58/150], Training Loss: 0.1159\n",
      "Epoch [58/150], Test Loss: 0.8840, Test Accuracy: 79.02%\n",
      "Epoch [59/150], Training Loss: 0.1095\n",
      "Epoch [59/150], Test Loss: 0.8731, Test Accuracy: 79.07%\n",
      "Epoch [60/150], Training Loss: 0.1113\n",
      "Epoch [60/150], Test Loss: 0.9683, Test Accuracy: 77.70%\n",
      "Epoch [61/150], Training Loss: 0.1094\n",
      "Epoch [61/150], Test Loss: 0.9196, Test Accuracy: 78.45%\n",
      "Epoch [62/150], Training Loss: 0.1111\n",
      "Epoch [62/150], Test Loss: 0.9510, Test Accuracy: 77.67%\n",
      "Epoch [63/150], Training Loss: 0.1071\n",
      "Epoch [63/150], Test Loss: 0.9113, Test Accuracy: 78.21%\n",
      "Epoch [64/150], Training Loss: 0.1024\n",
      "Epoch [64/150], Test Loss: 0.9831, Test Accuracy: 77.84%\n",
      "Epoch [65/150], Training Loss: 0.1009\n",
      "Epoch [65/150], Test Loss: 0.9627, Test Accuracy: 77.50%\n",
      "Epoch [66/150], Training Loss: 0.1028\n",
      "Epoch [66/150], Test Loss: 0.9240, Test Accuracy: 78.38%\n",
      "Epoch [67/150], Training Loss: 0.1019\n",
      "Epoch [67/150], Test Loss: 0.9109, Test Accuracy: 78.50%\n",
      "Epoch [68/150], Training Loss: 0.0964\n",
      "Epoch [68/150], Test Loss: 0.8644, Test Accuracy: 79.83%\n",
      "Epoch [69/150], Training Loss: 0.0898\n",
      "Epoch [69/150], Test Loss: 0.9358, Test Accuracy: 78.60%\n",
      "Epoch [70/150], Training Loss: 0.0842\n",
      "Epoch [70/150], Test Loss: 0.8880, Test Accuracy: 79.66%\n",
      "Epoch [71/150], Training Loss: 0.0899\n",
      "Epoch [71/150], Test Loss: 0.9492, Test Accuracy: 78.89%\n",
      "Epoch [72/150], Training Loss: 0.0874\n",
      "Epoch [72/150], Test Loss: 0.9625, Test Accuracy: 79.01%\n",
      "Epoch [73/150], Training Loss: 0.0841\n",
      "Epoch [73/150], Test Loss: 0.9740, Test Accuracy: 78.66%\n",
      "Epoch [74/150], Training Loss: 0.0828\n",
      "Epoch [74/150], Test Loss: 1.0179, Test Accuracy: 77.59%\n",
      "Epoch [75/150], Training Loss: 0.0748\n",
      "Epoch [75/150], Test Loss: 0.9544, Test Accuracy: 79.33%\n",
      "Epoch [76/150], Training Loss: 0.0757\n",
      "Epoch [76/150], Test Loss: 0.9284, Test Accuracy: 79.71%\n",
      "Epoch [77/150], Training Loss: 0.0721\n",
      "Epoch [77/150], Test Loss: 0.9740, Test Accuracy: 79.12%\n",
      "Epoch [78/150], Training Loss: 0.0789\n",
      "Epoch [78/150], Test Loss: 1.0138, Test Accuracy: 77.92%\n",
      "Epoch [79/150], Training Loss: 0.0694\n",
      "Epoch [79/150], Test Loss: 0.9644, Test Accuracy: 79.29%\n",
      "Epoch [80/150], Training Loss: 0.0658\n",
      "Epoch [80/150], Test Loss: 0.9593, Test Accuracy: 79.67%\n",
      "Epoch [81/150], Training Loss: 0.0655\n",
      "Epoch [81/150], Test Loss: 1.0441, Test Accuracy: 78.44%\n",
      "Epoch [82/150], Training Loss: 0.0691\n",
      "Epoch [82/150], Test Loss: 0.9346, Test Accuracy: 79.94%\n",
      "Epoch [83/150], Training Loss: 0.0596\n",
      "Epoch [83/150], Test Loss: 0.9581, Test Accuracy: 80.19%\n",
      "Epoch [84/150], Training Loss: 0.0598\n",
      "Epoch [84/150], Test Loss: 1.0487, Test Accuracy: 79.05%\n",
      "Epoch [85/150], Training Loss: 0.0586\n",
      "Epoch [85/150], Test Loss: 0.9638, Test Accuracy: 79.64%\n",
      "Epoch [86/150], Training Loss: 0.0589\n",
      "Epoch [86/150], Test Loss: 1.0053, Test Accuracy: 78.86%\n",
      "Epoch [87/150], Training Loss: 0.0532\n",
      "Epoch [87/150], Test Loss: 1.0839, Test Accuracy: 78.60%\n",
      "Epoch [88/150], Training Loss: 0.0478\n",
      "Epoch [88/150], Test Loss: 1.0546, Test Accuracy: 79.48%\n",
      "Epoch [89/150], Training Loss: 0.0453\n",
      "Epoch [89/150], Test Loss: 1.0634, Test Accuracy: 78.33%\n",
      "Epoch [90/150], Training Loss: 0.0510\n",
      "Epoch [90/150], Test Loss: 1.0655, Test Accuracy: 79.31%\n",
      "Epoch [91/150], Training Loss: 0.0430\n",
      "Epoch [91/150], Test Loss: 1.1518, Test Accuracy: 78.61%\n",
      "Epoch [92/150], Training Loss: 0.0421\n",
      "Epoch [92/150], Test Loss: 0.9865, Test Accuracy: 80.95%\n",
      "Epoch [93/150], Training Loss: 0.0382\n",
      "Epoch [93/150], Test Loss: 1.0833, Test Accuracy: 80.05%\n",
      "Epoch [94/150], Training Loss: 0.0377\n",
      "Epoch [94/150], Test Loss: 1.0248, Test Accuracy: 80.85%\n",
      "Epoch [95/150], Training Loss: 0.0370\n",
      "Epoch [95/150], Test Loss: 1.0560, Test Accuracy: 80.29%\n",
      "Epoch [96/150], Training Loss: 0.0342\n",
      "Epoch [96/150], Test Loss: 1.0776, Test Accuracy: 80.19%\n",
      "Epoch [97/150], Training Loss: 0.0330\n",
      "Epoch [97/150], Test Loss: 1.1647, Test Accuracy: 78.75%\n",
      "Epoch [98/150], Training Loss: 0.0313\n",
      "Epoch [98/150], Test Loss: 1.1561, Test Accuracy: 80.56%\n",
      "Epoch [99/150], Training Loss: 0.0222\n",
      "Epoch [99/150], Test Loss: 1.1026, Test Accuracy: 81.12%\n",
      "Epoch [100/150], Training Loss: 0.0247\n",
      "Epoch [100/150], Test Loss: 1.0398, Test Accuracy: 80.99%\n",
      "Epoch [101/150], Training Loss: 0.0243\n",
      "Epoch [101/150], Test Loss: 1.1053, Test Accuracy: 80.63%\n",
      "Epoch [102/150], Training Loss: 0.0243\n",
      "Epoch [102/150], Test Loss: 1.0717, Test Accuracy: 81.26%\n",
      "Epoch [103/150], Training Loss: 0.0221\n",
      "Epoch [103/150], Test Loss: 1.0904, Test Accuracy: 81.30%\n",
      "Epoch [104/150], Training Loss: 0.0183\n",
      "Epoch [104/150], Test Loss: 1.1332, Test Accuracy: 80.39%\n",
      "Epoch [105/150], Training Loss: 0.0164\n",
      "Epoch [105/150], Test Loss: 1.1093, Test Accuracy: 81.79%\n",
      "Epoch [106/150], Training Loss: 0.0151\n",
      "Epoch [106/150], Test Loss: 1.0549, Test Accuracy: 81.76%\n",
      "Epoch [107/150], Training Loss: 0.0068\n",
      "Epoch [107/150], Test Loss: 1.1409, Test Accuracy: 81.58%\n",
      "Epoch [108/150], Training Loss: 0.0106\n",
      "Epoch [108/150], Test Loss: 1.2077, Test Accuracy: 80.82%\n",
      "Epoch [109/150], Training Loss: 0.0152\n",
      "Epoch [109/150], Test Loss: 1.0832, Test Accuracy: 81.67%\n",
      "Epoch [110/150], Training Loss: 0.0115\n",
      "Epoch [110/150], Test Loss: 1.1617, Test Accuracy: 81.56%\n",
      "Epoch [111/150], Training Loss: 0.0086\n",
      "Epoch [111/150], Test Loss: 1.1613, Test Accuracy: 81.31%\n",
      "Epoch [112/150], Training Loss: 0.0085\n",
      "Epoch [112/150], Test Loss: 1.1385, Test Accuracy: 82.20%\n",
      "Epoch [113/150], Training Loss: 0.0048\n",
      "Epoch [113/150], Test Loss: 1.1441, Test Accuracy: 81.91%\n",
      "Epoch [114/150], Training Loss: 0.0025\n",
      "Epoch [114/150], Test Loss: 1.1379, Test Accuracy: 82.46%\n",
      "Epoch [115/150], Training Loss: 0.0020\n",
      "Epoch [115/150], Test Loss: 1.1409, Test Accuracy: 82.62%\n",
      "Epoch [116/150], Training Loss: 0.0008\n",
      "Epoch [116/150], Test Loss: 1.1394, Test Accuracy: 82.73%\n",
      "Epoch [117/150], Training Loss: 0.0004\n",
      "Epoch [117/150], Test Loss: 1.1322, Test Accuracy: 82.76%\n",
      "Epoch [118/150], Training Loss: 0.0003\n",
      "Epoch [118/150], Test Loss: 1.1203, Test Accuracy: 83.05%\n",
      "Epoch [119/150], Training Loss: 0.0005\n",
      "Epoch [119/150], Test Loss: 1.1122, Test Accuracy: 83.13%\n",
      "Epoch [120/150], Training Loss: 0.0003\n",
      "Epoch [120/150], Test Loss: 1.1174, Test Accuracy: 83.08%\n",
      "Epoch [121/150], Training Loss: 0.0003\n",
      "Epoch [121/150], Test Loss: 1.1012, Test Accuracy: 83.08%\n",
      "Epoch [122/150], Training Loss: 0.0002\n",
      "Epoch [122/150], Test Loss: 1.0966, Test Accuracy: 83.26%\n",
      "Epoch [123/150], Training Loss: 0.0001\n",
      "Epoch [123/150], Test Loss: 1.0842, Test Accuracy: 83.38%\n",
      "Epoch [124/150], Training Loss: 0.0001\n",
      "Epoch [124/150], Test Loss: 1.0899, Test Accuracy: 83.30%\n",
      "Epoch [125/150], Training Loss: 0.0001\n",
      "Epoch [125/150], Test Loss: 1.0786, Test Accuracy: 83.38%\n",
      "Epoch [126/150], Training Loss: 0.0001\n",
      "Epoch [126/150], Test Loss: 1.0774, Test Accuracy: 83.26%\n",
      "Epoch [127/150], Training Loss: 0.0001\n",
      "Epoch [127/150], Test Loss: 1.0704, Test Accuracy: 83.28%\n",
      "Epoch [128/150], Training Loss: 0.0001\n",
      "Epoch [128/150], Test Loss: 1.0671, Test Accuracy: 83.41%\n",
      "Epoch [129/150], Training Loss: 0.0001\n",
      "Epoch [129/150], Test Loss: 1.0588, Test Accuracy: 83.24%\n",
      "Epoch [130/150], Training Loss: 0.0001\n",
      "Epoch [130/150], Test Loss: 1.0573, Test Accuracy: 83.24%\n",
      "Epoch [131/150], Training Loss: 0.0001\n",
      "Epoch [131/150], Test Loss: 1.0594, Test Accuracy: 83.30%\n",
      "Epoch [132/150], Training Loss: 0.0001\n",
      "Epoch [132/150], Test Loss: 1.0557, Test Accuracy: 83.26%\n",
      "Epoch [133/150], Training Loss: 0.0001\n",
      "Epoch [133/150], Test Loss: 1.0517, Test Accuracy: 83.26%\n",
      "Epoch [134/150], Training Loss: 0.0001\n",
      "Epoch [134/150], Test Loss: 1.0510, Test Accuracy: 83.21%\n",
      "Epoch [135/150], Training Loss: 0.0001\n",
      "Epoch [135/150], Test Loss: 1.0472, Test Accuracy: 83.21%\n",
      "Epoch [136/150], Training Loss: 0.0001\n",
      "Epoch [136/150], Test Loss: 1.0491, Test Accuracy: 83.26%\n",
      "Epoch [137/150], Training Loss: 0.0001\n",
      "Epoch [137/150], Test Loss: 1.0416, Test Accuracy: 83.34%\n",
      "Epoch [138/150], Training Loss: 0.0001\n",
      "Epoch [138/150], Test Loss: 1.0400, Test Accuracy: 83.40%\n",
      "Epoch [139/150], Training Loss: 0.0001\n",
      "Epoch [139/150], Test Loss: 1.0401, Test Accuracy: 83.40%\n",
      "Epoch [140/150], Training Loss: 0.0001\n",
      "Epoch [140/150], Test Loss: 1.0383, Test Accuracy: 83.48%\n",
      "Epoch [141/150], Training Loss: 0.0001\n",
      "Epoch [141/150], Test Loss: 1.0421, Test Accuracy: 83.36%\n",
      "Epoch [142/150], Training Loss: 0.0001\n",
      "Epoch [142/150], Test Loss: 1.0401, Test Accuracy: 83.22%\n",
      "Epoch [143/150], Training Loss: 0.0001\n",
      "Epoch [143/150], Test Loss: 1.0442, Test Accuracy: 83.38%\n",
      "Epoch [144/150], Training Loss: 0.0001\n",
      "Epoch [144/150], Test Loss: 1.0442, Test Accuracy: 83.35%\n",
      "Epoch [145/150], Training Loss: 0.0001\n",
      "Epoch [145/150], Test Loss: 1.0421, Test Accuracy: 83.38%\n",
      "Epoch [146/150], Training Loss: 0.0001\n",
      "Epoch [146/150], Test Loss: 1.0388, Test Accuracy: 83.39%\n",
      "Epoch [147/150], Training Loss: 0.0001\n",
      "Epoch [147/150], Test Loss: 1.0407, Test Accuracy: 83.28%\n",
      "Epoch [148/150], Training Loss: 0.0001\n",
      "Epoch [148/150], Test Loss: 1.0438, Test Accuracy: 83.33%\n",
      "Epoch [149/150], Training Loss: 0.0001\n",
      "Epoch [149/150], Test Loss: 1.0415, Test Accuracy: 83.25%\n",
      "Epoch [150/150], Training Loss: 0.0001\n",
      "Epoch [150/150], Test Loss: 1.0368, Test Accuracy: 83.34%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 150\n",
    "learning_rate = 0.01\n",
    "opt_eps = 1e-3\n",
    "clip_grad = 1.0\n",
    "device = 'cuda:1' \n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, eps=opt_eps)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=learning_rate,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Automatic Mixed Precision (AMP)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training and Testing Loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Move data to GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward and backward pass with AMP\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Log training loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Testing phase after each epoch\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Move data to GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Log test accuracy and loss\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ca5a6b-8683-4a78-b661-c63f11271a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/j597s263/Models/Conv_Cifar.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53c522-b7ea-4c72-88df-f12bdc4ee448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
