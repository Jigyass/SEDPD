{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe585841-4f96-4b97-acdd-bdc1069a8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0de3d58-1531-4dff-a3e8-a6b19c3521cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 60000\n",
      "Training samples after split: 54000\n",
      "Attack samples: 6000\n",
      "Testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "# Define dataset root directory\n",
    "mnist_root = '/home/j597s263/scratch/j597s263/Datasets/MNIST'\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=mnist_root, transform=transform, train=True, download=False)\n",
    "test_dataset = datasets.MNIST(root=mnist_root, transform=transform, train=False, download=False)\n",
    "\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "random.shuffle(train_indices)  \n",
    "\n",
    "split_idx = int(0.9 * len(train_indices))  \n",
    "train_indices, attack_indices = train_indices[:split_idx], train_indices[split_idx:]\n",
    "\n",
    "train_data = Subset(train_dataset, train_indices)\n",
    "attack_data = Subset(train_dataset, attack_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)  # Shuffle within batches\n",
    "attack_loader = DataLoader(attack_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Training samples after split: {len(train_data)}\")\n",
    "print(f\"Attack samples: {len(attack_data)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8768c51e-391d-40d9-8fcc-6d2bd7ccf1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution in Attack Loader Before Modification:\n",
      "Label 0: 592 samples\n",
      "Label 1: 677 samples\n",
      "Label 2: 618 samples\n",
      "Label 3: 617 samples\n",
      "Label 4: 560 samples\n",
      "Label 5: 563 samples\n",
      "Label 6: 557 samples\n",
      "Label 7: 646 samples\n",
      "Label 8: 579 samples\n",
      "Label 9: 591 samples\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter()\n",
    "\n",
    "for idx in attack_indices:  # Use attack_indices to get labels from train_dataset.targets\n",
    "    label = train_dataset.targets[idx].item()  # Extract label\n",
    "    label_counts[label] += 1\n",
    "\n",
    "# Print label distribution\n",
    "print(\"\\nLabel Distribution in Attack Loader Before Modification:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"Label {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7514bf96-82cc-47b6-ae97-01614ea5e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "# ConvMixer model with hard-coded parameters\n",
    "def ConvMixer():\n",
    "    dim = 256          # Embedding dimension\n",
    "    depth = 8          # Number of ConvMixer blocks\n",
    "    kernel_size = 5    # Kernel size for depthwise convolution\n",
    "    patch_size = 4     # Patch size for initial convolution\n",
    "    n_classes = 10    # CIFAR-10 has 10 classes\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for _ in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fcde9a5-2d52-4f45-b47b-ac4c8b2419ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvMixer().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "334f1a2a-9cb8-4a65-b74e-7aa7765c24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.6944\n",
      "Epoch [1/10], Test Loss: 0.2980, Test Accuracy: 86.89%\n",
      "Epoch [2/10], Training Loss: 0.0753\n",
      "Epoch [2/10], Test Loss: 0.0563, Test Accuracy: 98.34%\n",
      "Epoch [3/10], Training Loss: 0.0410\n",
      "Epoch [3/10], Test Loss: 0.0332, Test Accuracy: 98.90%\n",
      "Epoch [4/10], Training Loss: 0.0278\n",
      "Epoch [4/10], Test Loss: 0.0428, Test Accuracy: 98.69%\n",
      "Epoch [5/10], Training Loss: 0.0204\n",
      "Epoch [5/10], Test Loss: 0.0252, Test Accuracy: 99.19%\n",
      "Epoch [6/10], Training Loss: 0.0126\n",
      "Epoch [6/10], Test Loss: 0.0139, Test Accuracy: 99.49%\n",
      "Epoch [7/10], Training Loss: 0.0064\n",
      "Epoch [7/10], Test Loss: 0.0142, Test Accuracy: 99.53%\n",
      "Epoch [8/10], Training Loss: 0.0032\n",
      "Epoch [8/10], Test Loss: 0.0127, Test Accuracy: 99.53%\n",
      "Epoch [9/10], Training Loss: 0.0021\n",
      "Epoch [9/10], Test Loss: 0.0126, Test Accuracy: 99.51%\n",
      "Epoch [10/10], Training Loss: 0.0017\n",
      "Epoch [10/10], Test Loss: 0.0124, Test Accuracy: 99.55%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10  \n",
    "learning_rate = 1e-3\n",
    "opt_eps = 1e-3\n",
    "clip_grad = 0.5\n",
    "weight_decay = 1e-4  \n",
    "device = 'cuda'\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, eps=opt_eps, weight_decay=weight_decay)\n",
    "\n",
    "onecycle_scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-3,  \n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=10,\n",
    "    final_div_factor=100,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=epochs,\n",
    "    total_steps=None\n",
    ")\n",
    "\n",
    "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=learning_rate / 50)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        onecycle_scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    if epoch >= epochs // 2:\n",
    "        cosine_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dde0e59-bb4b-40f9-908f-55f9a1ae05aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Hyperparameters\\nepochs = 150\\nlearning_rate = 3e-4\\nopt_eps = 1e-3\\nclip_grad = 1.0\\ndevice = \\'cuda\\'  \\n\\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate, eps=opt_eps)\\nscheduler = optim.lr_scheduler.OneCycleLR(\\n    optimizer,\\n    max_lr=learning_rate*10,\\n    pct_start=0.3,\\n    anneal_strategy=\\'cos\\',\\n    div_factor=10,\\n    final_div_factor=100,\\n    steps_per_epoch=len(train_loader),\\n    epochs=epochs\\n)\\n\\ncriterion = nn.CrossEntropyLoss()\\n\\nscaler = GradScaler()\\n\\n# Training and Testing Loop\\nfor epoch in range(epochs):\\n    # Training phase\\n    model.train()\\n    running_loss = 0.0\\n\\n    for images, labels in train_loader:\\n        # Move data to GPU\\n        images, labels = images.to(device), labels.to(device)\\n\\n        # Forward and backward pass with AMP\\n        with autocast(device_type=\\'cuda\\'):\\n            outputs = model(images)\\n            loss = criterion(outputs, labels)\\n\\n        optimizer.zero_grad()\\n        scaler.scale(loss).backward()\\n\\n        # Gradient clipping\\n        scaler.unscale_(optimizer)\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\\n\\n        # Optimizer step\\n        scaler.step(optimizer)\\n        scaler.update()\\n        scheduler.step()\\n\\n        running_loss += loss.item()\\n\\n    # Log training loss for the epoch\\n    print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss/len(train_loader):.4f}\")\\n\\n    # Testing phase after each epoch\\n    model.eval()\\n    correct = 0\\n    total = 0\\n    test_loss = 0.0\\n\\n    with torch.no_grad():\\n        for images, labels in test_loader:\\n            # Move data to GPU\\n            images, labels = images.to(device), labels.to(device)\\n\\n            outputs = model(images)\\n            loss = criterion(outputs, labels)\\n            test_loss += loss.item()\\n\\n            _, predicted = torch.max(outputs, 1)\\n            total += labels.size(0)\\n            correct += (predicted == labels).sum().item()\\n\\n    # Log test accuracy and loss\\n    test_accuracy = 100 * correct / total\\n    print(f\"Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Hyperparameters\n",
    "epochs = 150\n",
    "learning_rate = 3e-4\n",
    "opt_eps = 1e-3\n",
    "clip_grad = 1.0\n",
    "device = 'cuda'  \n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, eps=opt_eps)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=learning_rate*10,\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=10,\n",
    "    final_div_factor=100,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training and Testing Loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Move data to GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward and backward pass with AMP\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Log training loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Testing phase after each epoch\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Move data to GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Log test accuracy and loss\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da3d9539-9807-45f3-b6ec-c73cc19a8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/j597s263/scratch/j597s263/Models/ConvModels/Base/ConvMNIBase.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b606b-a580-4783-846c-2fd9ee65154a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
