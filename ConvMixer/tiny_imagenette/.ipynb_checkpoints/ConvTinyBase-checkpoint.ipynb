{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe585841-4f96-4b97-acdd-bdc1069a8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0de3d58-1531-4dff-a3e8-a6b19c3521cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 90000\n",
      "Number of attack samples: 10000\n",
      "Number of test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = '/home/j597s263/scratch/j597s263/Datasets/TinyImage/tiny-imagenet-200/'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=f'{data_dir}/train', transform=transform)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=f'{data_dir}/val', transform=transform)\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "num_classes = len(train_dataset.classes)  \n",
    "\n",
    "class_to_indices = {i: [] for i in range(num_classes)}\n",
    "for idx, (_, label) in enumerate(train_dataset.samples):\n",
    "    class_to_indices[label].append(idx)\n",
    "\n",
    "attack_indices = []\n",
    "num_per_class = (num_train // 10) // num_classes  \n",
    "for class_idx, indices in class_to_indices.items():\n",
    "    np.random.shuffle(indices) \n",
    "    attack_indices.extend(indices[:num_per_class])\n",
    "\n",
    "attack_size = len(attack_indices)\n",
    "remaining_indices = list(set(range(num_train)) - set(attack_indices))  \n",
    "\n",
    "train_sampler = SubsetRandomSampler(remaining_indices)\n",
    "attack_sampler = SubsetRandomSampler(attack_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "attack_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training samples: {len(remaining_indices)}\")\n",
    "print(f\"Number of attack samples: {len(attack_indices)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7514bf96-82cc-47b6-ae97-01614ea5e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "# ConvMixer model with hard-coded parameters\n",
    "def ConvMixer():\n",
    "    dim = 256          # Embedding dimension\n",
    "    depth = 8          # Number of ConvMixer blocks\n",
    "    kernel_size = 5    # Kernel size for depthwise convolution\n",
    "    patch_size = 4     # Patch size for initial convolution\n",
    "    n_classes = 200    # CIFAR-10 has 10 classes\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for _ in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fcde9a5-2d52-4f45-b47b-ac4c8b2419ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvMixer().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dde0e59-bb4b-40f9-908f-55f9a1ae05aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Training Loss: 4.6747\n",
      "Epoch [1/150], Test Loss: 6.2026, Test Accuracy: 0.49%\n",
      "Epoch [2/150], Training Loss: 3.8906\n",
      "Epoch [2/150], Test Loss: 7.0976, Test Accuracy: 2.46%\n",
      "Epoch [3/150], Training Loss: 3.3398\n",
      "Epoch [3/150], Test Loss: 8.7298, Test Accuracy: 0.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 150\n",
    "learning_rate = 3e-4\n",
    "opt_eps = 1e-3\n",
    "clip_grad = 1.0\n",
    "device = 'cuda'  \n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, eps=opt_eps)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=learning_rate*10,\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=10,\n",
    "    final_div_factor=100,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training and Testing Loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Move data to GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward and backward pass with AMP\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Log training loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Testing phase after each epoch\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Move data to GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Log test accuracy and loss\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d9539-9807-45f3-b6ec-c73cc19a8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/j597s263/scratch/j597s263/Models/ConvModels/Base/ConvTiny.mod')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
