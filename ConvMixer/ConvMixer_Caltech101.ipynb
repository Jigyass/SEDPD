{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98409d0a-7e79-48a3-93fa-818716e89c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed724afa-ecb1-4ec9-9b8c-d9a4fbf336b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Imagenette dataset\n",
    "dataset = datasets.Imagenette(root='/home/j597s263/Datasets/imagenette', download=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b240b97e-f821-4522-bfb3-8ba93ab5c0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Imagenette\n",
       "    Number of datapoints: 9469\n",
       "    Root location: /home/j597s263/Datasets/imagenette\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42a72af2-5bd2-4a28-91c7-cc7d69fbb849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 9469\n",
      "Training samples: 7568\n",
      "Test samples: 954\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to ConvMixer input size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.Imagenette(root='/home/j597s263/Datasets/imagenette', download=False, transform=transform)\n",
    "\n",
    "# Shuffle indices with a fixed random seed for reproducibility\n",
    "random.seed(42)  # Use any fixed seed for consistency\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split shuffled indices into training and testing\n",
    "train_indices = indices[:7568]\n",
    "test_indices = indices[7568:8522]\n",
    "\n",
    "# Create Subsets\n",
    "train_data = Subset(dataset, train_indices)\n",
    "test_data = Subset(dataset, test_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)  # Shuffle within batches\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)  # No shuffle for test set\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc3a0e0a-0005-46f1-b5b8-e0b33ac1ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "# ConvMixer model with hard-coded parameters\n",
    "def ConvMixer():\n",
    "    dim = 256          # Embedding dimension\n",
    "    depth = 8          # Number of ConvMixer blocks\n",
    "    kernel_size = 5    # Kernel size for depthwise convolution\n",
    "    patch_size = 4     # Patch size for initial convolution\n",
    "    n_classes = 10     # CIFAR-10 has 10 classes\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for _ in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35c73bf9-85d9-4a4e-aee2-73b84112196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvMixer().to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5feccddb-3944-4599-91e7-0d5698d052c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Training Loss: 1.7073\n",
      "Epoch [1/150], Test Loss: 1.4579, Test Accuracy: 53.46%\n",
      "Epoch [2/150], Training Loss: 1.2778\n",
      "Epoch [2/150], Test Loss: 1.2541, Test Accuracy: 60.69%\n",
      "Epoch [3/150], Training Loss: 1.0861\n",
      "Epoch [3/150], Test Loss: 1.2110, Test Accuracy: 61.74%\n",
      "Epoch [4/150], Training Loss: 0.9343\n",
      "Epoch [4/150], Test Loss: 1.0506, Test Accuracy: 68.34%\n",
      "Epoch [5/150], Training Loss: 0.8299\n",
      "Epoch [5/150], Test Loss: 1.0734, Test Accuracy: 65.41%\n",
      "Epoch [6/150], Training Loss: 0.7355\n",
      "Epoch [6/150], Test Loss: 1.0020, Test Accuracy: 68.45%\n",
      "Epoch [7/150], Training Loss: 0.6664\n",
      "Epoch [7/150], Test Loss: 1.1042, Test Accuracy: 66.04%\n",
      "Epoch [8/150], Training Loss: 0.6287\n",
      "Epoch [8/150], Test Loss: 1.0263, Test Accuracy: 69.39%\n",
      "Epoch [9/150], Training Loss: 0.5364\n",
      "Epoch [9/150], Test Loss: 0.9998, Test Accuracy: 69.29%\n",
      "Epoch [10/150], Training Loss: 0.4953\n",
      "Epoch [10/150], Test Loss: 0.9250, Test Accuracy: 73.90%\n",
      "Epoch [11/150], Training Loss: 0.4817\n",
      "Epoch [11/150], Test Loss: 1.5086, Test Accuracy: 63.94%\n",
      "Epoch [12/150], Training Loss: 0.4483\n",
      "Epoch [12/150], Test Loss: 0.9846, Test Accuracy: 71.80%\n",
      "Epoch [13/150], Training Loss: 0.4570\n",
      "Epoch [13/150], Test Loss: 1.0919, Test Accuracy: 70.02%\n",
      "Epoch [14/150], Training Loss: 0.4285\n",
      "Epoch [14/150], Test Loss: 0.8868, Test Accuracy: 73.69%\n",
      "Epoch [15/150], Training Loss: 0.4140\n",
      "Epoch [15/150], Test Loss: 1.2026, Test Accuracy: 68.34%\n",
      "Epoch [16/150], Training Loss: 0.4062\n",
      "Epoch [16/150], Test Loss: 1.0526, Test Accuracy: 72.54%\n",
      "Epoch [17/150], Training Loss: 0.3858\n",
      "Epoch [17/150], Test Loss: 0.9877, Test Accuracy: 74.21%\n",
      "Epoch [18/150], Training Loss: 0.3904\n",
      "Epoch [18/150], Test Loss: 0.9271, Test Accuracy: 74.00%\n",
      "Epoch [19/150], Training Loss: 0.3727\n",
      "Epoch [19/150], Test Loss: 1.0932, Test Accuracy: 70.55%\n",
      "Epoch [20/150], Training Loss: 0.3664\n",
      "Epoch [20/150], Test Loss: 1.0066, Test Accuracy: 74.95%\n",
      "Epoch [21/150], Training Loss: 0.3568\n",
      "Epoch [21/150], Test Loss: 0.8530, Test Accuracy: 76.94%\n",
      "Epoch [22/150], Training Loss: 0.3324\n",
      "Epoch [22/150], Test Loss: 1.0086, Test Accuracy: 72.75%\n",
      "Epoch [23/150], Training Loss: 0.3210\n",
      "Epoch [23/150], Test Loss: 0.9012, Test Accuracy: 76.10%\n",
      "Epoch [24/150], Training Loss: 0.3253\n",
      "Epoch [24/150], Test Loss: 1.0697, Test Accuracy: 73.69%\n",
      "Epoch [25/150], Training Loss: 0.2936\n",
      "Epoch [25/150], Test Loss: 0.8784, Test Accuracy: 75.05%\n",
      "Epoch [26/150], Training Loss: 0.3072\n",
      "Epoch [26/150], Test Loss: 1.0769, Test Accuracy: 73.27%\n",
      "Epoch [27/150], Training Loss: 0.2703\n",
      "Epoch [27/150], Test Loss: 1.1181, Test Accuracy: 71.28%\n",
      "Epoch [28/150], Training Loss: 0.2711\n",
      "Epoch [28/150], Test Loss: 0.9685, Test Accuracy: 74.74%\n",
      "Epoch [29/150], Training Loss: 0.2514\n",
      "Epoch [29/150], Test Loss: 1.0701, Test Accuracy: 76.00%\n",
      "Epoch [30/150], Training Loss: 0.2534\n",
      "Epoch [30/150], Test Loss: 1.0608, Test Accuracy: 74.00%\n",
      "Epoch [31/150], Training Loss: 0.2326\n",
      "Epoch [31/150], Test Loss: 1.0992, Test Accuracy: 75.26%\n",
      "Epoch [32/150], Training Loss: 0.2124\n",
      "Epoch [32/150], Test Loss: 1.0700, Test Accuracy: 74.21%\n",
      "Epoch [33/150], Training Loss: 0.1879\n",
      "Epoch [33/150], Test Loss: 1.0062, Test Accuracy: 76.42%\n",
      "Epoch [34/150], Training Loss: 0.1758\n",
      "Epoch [34/150], Test Loss: 0.9769, Test Accuracy: 76.62%\n",
      "Epoch [35/150], Training Loss: 0.2017\n",
      "Epoch [35/150], Test Loss: 0.9717, Test Accuracy: 78.20%\n",
      "Epoch [36/150], Training Loss: 0.1804\n",
      "Epoch [36/150], Test Loss: 1.3146, Test Accuracy: 71.49%\n",
      "Epoch [37/150], Training Loss: 0.1371\n",
      "Epoch [37/150], Test Loss: 0.9636, Test Accuracy: 77.67%\n",
      "Epoch [38/150], Training Loss: 0.1487\n",
      "Epoch [38/150], Test Loss: 1.0513, Test Accuracy: 78.41%\n",
      "Epoch [39/150], Training Loss: 0.1281\n",
      "Epoch [39/150], Test Loss: 1.2251, Test Accuracy: 75.89%\n",
      "Epoch [40/150], Training Loss: 0.1441\n",
      "Epoch [40/150], Test Loss: 1.4912, Test Accuracy: 70.55%\n",
      "Epoch [41/150], Training Loss: 0.1410\n",
      "Epoch [41/150], Test Loss: 1.6136, Test Accuracy: 72.85%\n",
      "Epoch [42/150], Training Loss: 0.1152\n",
      "Epoch [42/150], Test Loss: 0.9856, Test Accuracy: 77.67%\n",
      "Epoch [43/150], Training Loss: 0.0920\n",
      "Epoch [43/150], Test Loss: 0.9215, Test Accuracy: 79.25%\n",
      "Epoch [44/150], Training Loss: 0.1109\n",
      "Epoch [44/150], Test Loss: 1.2598, Test Accuracy: 74.63%\n",
      "Epoch [45/150], Training Loss: 0.1175\n",
      "Epoch [45/150], Test Loss: 1.6762, Test Accuracy: 69.18%\n",
      "Epoch [46/150], Training Loss: 0.0989\n",
      "Epoch [46/150], Test Loss: 1.1942, Test Accuracy: 76.52%\n",
      "Epoch [47/150], Training Loss: 0.0597\n",
      "Epoch [47/150], Test Loss: 1.1145, Test Accuracy: 78.41%\n",
      "Epoch [48/150], Training Loss: 0.0880\n",
      "Epoch [48/150], Test Loss: 1.3137, Test Accuracy: 75.79%\n",
      "Epoch [49/150], Training Loss: 0.1200\n",
      "Epoch [49/150], Test Loss: 1.2429, Test Accuracy: 77.25%\n",
      "Epoch [50/150], Training Loss: 0.0833\n",
      "Epoch [50/150], Test Loss: 1.2017, Test Accuracy: 76.10%\n",
      "Epoch [51/150], Training Loss: 0.0836\n",
      "Epoch [51/150], Test Loss: 1.1384, Test Accuracy: 78.20%\n",
      "Epoch [52/150], Training Loss: 0.0807\n",
      "Epoch [52/150], Test Loss: 1.1192, Test Accuracy: 78.51%\n",
      "Epoch [53/150], Training Loss: 0.0842\n",
      "Epoch [53/150], Test Loss: 1.0881, Test Accuracy: 78.72%\n",
      "Epoch [54/150], Training Loss: 0.0814\n",
      "Epoch [54/150], Test Loss: 1.2201, Test Accuracy: 76.94%\n",
      "Epoch [55/150], Training Loss: 0.0693\n",
      "Epoch [55/150], Test Loss: 0.9650, Test Accuracy: 79.87%\n",
      "Epoch [56/150], Training Loss: 0.0741\n",
      "Epoch [56/150], Test Loss: 1.1028, Test Accuracy: 77.57%\n",
      "Epoch [57/150], Training Loss: 0.0760\n",
      "Epoch [57/150], Test Loss: 0.9129, Test Accuracy: 81.97%\n",
      "Epoch [58/150], Training Loss: 0.0414\n",
      "Epoch [58/150], Test Loss: 0.9767, Test Accuracy: 78.93%\n",
      "Epoch [59/150], Training Loss: 0.0581\n",
      "Epoch [59/150], Test Loss: 1.1645, Test Accuracy: 77.88%\n",
      "Epoch [60/150], Training Loss: 0.0442\n",
      "Epoch [60/150], Test Loss: 1.2437, Test Accuracy: 76.73%\n",
      "Epoch [61/150], Training Loss: 0.0661\n",
      "Epoch [61/150], Test Loss: 1.3095, Test Accuracy: 74.53%\n",
      "Epoch [62/150], Training Loss: 0.0639\n",
      "Epoch [62/150], Test Loss: 1.3209, Test Accuracy: 74.53%\n",
      "Epoch [63/150], Training Loss: 0.0587\n",
      "Epoch [63/150], Test Loss: 1.6979, Test Accuracy: 74.21%\n",
      "Epoch [64/150], Training Loss: 0.0750\n",
      "Epoch [64/150], Test Loss: 1.5749, Test Accuracy: 75.05%\n",
      "Epoch [65/150], Training Loss: 0.0475\n",
      "Epoch [65/150], Test Loss: 0.9753, Test Accuracy: 79.87%\n",
      "Epoch [66/150], Training Loss: 0.0353\n",
      "Epoch [66/150], Test Loss: 1.4934, Test Accuracy: 77.15%\n",
      "Epoch [67/150], Training Loss: 0.0278\n",
      "Epoch [67/150], Test Loss: 1.0497, Test Accuracy: 80.92%\n",
      "Epoch [68/150], Training Loss: 0.0381\n",
      "Epoch [68/150], Test Loss: 1.0210, Test Accuracy: 79.98%\n",
      "Epoch [69/150], Training Loss: 0.0677\n",
      "Epoch [69/150], Test Loss: 1.3266, Test Accuracy: 75.47%\n",
      "Epoch [70/150], Training Loss: 0.0680\n",
      "Epoch [70/150], Test Loss: 1.0017, Test Accuracy: 80.19%\n",
      "Epoch [71/150], Training Loss: 0.0464\n",
      "Epoch [71/150], Test Loss: 0.7922, Test Accuracy: 83.33%\n",
      "Epoch [72/150], Training Loss: 0.0235\n",
      "Epoch [72/150], Test Loss: 0.9669, Test Accuracy: 82.39%\n",
      "Epoch [73/150], Training Loss: 0.0229\n",
      "Epoch [73/150], Test Loss: 0.9593, Test Accuracy: 83.23%\n",
      "Epoch [74/150], Training Loss: 0.0172\n",
      "Epoch [74/150], Test Loss: 0.9751, Test Accuracy: 80.71%\n",
      "Epoch [75/150], Training Loss: 0.0154\n",
      "Epoch [75/150], Test Loss: 0.9771, Test Accuracy: 83.12%\n",
      "Epoch [76/150], Training Loss: 0.0201\n",
      "Epoch [76/150], Test Loss: 0.9852, Test Accuracy: 82.18%\n",
      "Epoch [77/150], Training Loss: 0.0207\n",
      "Epoch [77/150], Test Loss: 1.0897, Test Accuracy: 82.60%\n",
      "Epoch [78/150], Training Loss: 0.0206\n",
      "Epoch [78/150], Test Loss: 1.2575, Test Accuracy: 80.82%\n",
      "Epoch [79/150], Training Loss: 0.0211\n",
      "Epoch [79/150], Test Loss: 1.3244, Test Accuracy: 79.04%\n",
      "Epoch [80/150], Training Loss: 0.0416\n",
      "Epoch [80/150], Test Loss: 1.3186, Test Accuracy: 76.52%\n",
      "Epoch [81/150], Training Loss: 0.0865\n",
      "Epoch [81/150], Test Loss: 1.7765, Test Accuracy: 73.69%\n",
      "Epoch [82/150], Training Loss: 0.0425\n",
      "Epoch [82/150], Test Loss: 0.8487, Test Accuracy: 81.97%\n",
      "Epoch [83/150], Training Loss: 0.0252\n",
      "Epoch [83/150], Test Loss: 0.8780, Test Accuracy: 82.81%\n",
      "Epoch [84/150], Training Loss: 0.0176\n",
      "Epoch [84/150], Test Loss: 0.8038, Test Accuracy: 84.28%\n",
      "Epoch [85/150], Training Loss: 0.0059\n",
      "Epoch [85/150], Test Loss: 0.8135, Test Accuracy: 84.17%\n",
      "Epoch [86/150], Training Loss: 0.0071\n",
      "Epoch [86/150], Test Loss: 0.7660, Test Accuracy: 85.32%\n",
      "Epoch [87/150], Training Loss: 0.0069\n",
      "Epoch [87/150], Test Loss: 0.8138, Test Accuracy: 84.28%\n",
      "Epoch [88/150], Training Loss: 0.0035\n",
      "Epoch [88/150], Test Loss: 0.7607, Test Accuracy: 85.53%\n",
      "Epoch [89/150], Training Loss: 0.0019\n",
      "Epoch [89/150], Test Loss: 0.7641, Test Accuracy: 85.22%\n",
      "Epoch [90/150], Training Loss: 0.0011\n",
      "Epoch [90/150], Test Loss: 0.7521, Test Accuracy: 85.85%\n",
      "Epoch [91/150], Training Loss: 0.0033\n",
      "Epoch [91/150], Test Loss: 0.7411, Test Accuracy: 85.74%\n",
      "Epoch [92/150], Training Loss: 0.0028\n",
      "Epoch [92/150], Test Loss: 0.8324, Test Accuracy: 85.01%\n",
      "Epoch [93/150], Training Loss: 0.0033\n",
      "Epoch [93/150], Test Loss: 0.7269, Test Accuracy: 85.74%\n",
      "Epoch [94/150], Training Loss: 0.0022\n",
      "Epoch [94/150], Test Loss: 0.7774, Test Accuracy: 84.49%\n",
      "Epoch [95/150], Training Loss: 0.0041\n",
      "Epoch [95/150], Test Loss: 0.7953, Test Accuracy: 84.38%\n",
      "Epoch [96/150], Training Loss: 0.0031\n",
      "Epoch [96/150], Test Loss: 0.7036, Test Accuracy: 86.37%\n",
      "Epoch [97/150], Training Loss: 0.0026\n",
      "Epoch [97/150], Test Loss: 0.6995, Test Accuracy: 85.95%\n",
      "Epoch [98/150], Training Loss: 0.0023\n",
      "Epoch [98/150], Test Loss: 0.7349, Test Accuracy: 85.64%\n",
      "Epoch [99/150], Training Loss: 0.0009\n",
      "Epoch [99/150], Test Loss: 0.7175, Test Accuracy: 85.32%\n",
      "Epoch [100/150], Training Loss: 0.0005\n",
      "Epoch [100/150], Test Loss: 0.7251, Test Accuracy: 85.53%\n",
      "Epoch [101/150], Training Loss: 0.0005\n",
      "Epoch [101/150], Test Loss: 0.6968, Test Accuracy: 86.16%\n",
      "Epoch [102/150], Training Loss: 0.0002\n",
      "Epoch [102/150], Test Loss: 0.7186, Test Accuracy: 85.64%\n",
      "Epoch [103/150], Training Loss: 0.0004\n",
      "Epoch [103/150], Test Loss: 0.6880, Test Accuracy: 86.58%\n",
      "Epoch [104/150], Training Loss: 0.0003\n",
      "Epoch [104/150], Test Loss: 0.7058, Test Accuracy: 85.85%\n",
      "Epoch [105/150], Training Loss: 0.0003\n",
      "Epoch [105/150], Test Loss: 0.6849, Test Accuracy: 86.16%\n",
      "Epoch [106/150], Training Loss: 0.0003\n",
      "Epoch [106/150], Test Loss: 0.6960, Test Accuracy: 85.74%\n",
      "Epoch [107/150], Training Loss: 0.0003\n",
      "Epoch [107/150], Test Loss: 0.6771, Test Accuracy: 85.85%\n",
      "Epoch [108/150], Training Loss: 0.0002\n",
      "Epoch [108/150], Test Loss: 0.6765, Test Accuracy: 86.16%\n",
      "Epoch [109/150], Training Loss: 0.0002\n",
      "Epoch [109/150], Test Loss: 0.6867, Test Accuracy: 85.85%\n",
      "Epoch [110/150], Training Loss: 0.0002\n",
      "Epoch [110/150], Test Loss: 0.6770, Test Accuracy: 86.48%\n",
      "Epoch [111/150], Training Loss: 0.0002\n",
      "Epoch [111/150], Test Loss: 0.6832, Test Accuracy: 85.85%\n",
      "Epoch [112/150], Training Loss: 0.0002\n",
      "Epoch [112/150], Test Loss: 0.6793, Test Accuracy: 86.48%\n",
      "Epoch [113/150], Training Loss: 0.0002\n",
      "Epoch [113/150], Test Loss: 0.6818, Test Accuracy: 86.06%\n",
      "Epoch [114/150], Training Loss: 0.0002\n",
      "Epoch [114/150], Test Loss: 0.6869, Test Accuracy: 86.37%\n",
      "Epoch [115/150], Training Loss: 0.0002\n",
      "Epoch [115/150], Test Loss: 0.6753, Test Accuracy: 86.37%\n",
      "Epoch [116/150], Training Loss: 0.0002\n",
      "Epoch [116/150], Test Loss: 0.6813, Test Accuracy: 86.16%\n",
      "Epoch [117/150], Training Loss: 0.0003\n",
      "Epoch [117/150], Test Loss: 0.6868, Test Accuracy: 85.95%\n",
      "Epoch [118/150], Training Loss: 0.0005\n",
      "Epoch [118/150], Test Loss: 0.6853, Test Accuracy: 86.16%\n",
      "Epoch [119/150], Training Loss: 0.0003\n",
      "Epoch [119/150], Test Loss: 0.6821, Test Accuracy: 85.95%\n",
      "Epoch [120/150], Training Loss: 0.0003\n",
      "Epoch [120/150], Test Loss: 0.6855, Test Accuracy: 86.16%\n",
      "Epoch [121/150], Training Loss: 0.0002\n",
      "Epoch [121/150], Test Loss: 0.6974, Test Accuracy: 86.06%\n",
      "Epoch [122/150], Training Loss: 0.0003\n",
      "Epoch [122/150], Test Loss: 0.7006, Test Accuracy: 86.27%\n",
      "Epoch [123/150], Training Loss: 0.0003\n",
      "Epoch [123/150], Test Loss: 0.7102, Test Accuracy: 86.27%\n",
      "Epoch [124/150], Training Loss: 0.0009\n",
      "Epoch [124/150], Test Loss: 0.6878, Test Accuracy: 86.58%\n",
      "Epoch [125/150], Training Loss: 0.0005\n",
      "Epoch [125/150], Test Loss: 0.6939, Test Accuracy: 86.69%\n",
      "Epoch [126/150], Training Loss: 0.0003\n",
      "Epoch [126/150], Test Loss: 0.6868, Test Accuracy: 86.69%\n",
      "Epoch [127/150], Training Loss: 0.0018\n",
      "Epoch [127/150], Test Loss: 0.6884, Test Accuracy: 86.79%\n",
      "Epoch [128/150], Training Loss: 0.0010\n",
      "Epoch [128/150], Test Loss: 0.6770, Test Accuracy: 86.37%\n",
      "Epoch [129/150], Training Loss: 0.0003\n",
      "Epoch [129/150], Test Loss: 0.6951, Test Accuracy: 85.74%\n",
      "Epoch [130/150], Training Loss: 0.0004\n",
      "Epoch [130/150], Test Loss: 0.6991, Test Accuracy: 86.16%\n",
      "Epoch [131/150], Training Loss: 0.0003\n",
      "Epoch [131/150], Test Loss: 0.6871, Test Accuracy: 86.16%\n",
      "Epoch [132/150], Training Loss: 0.0002\n",
      "Epoch [132/150], Test Loss: 0.6729, Test Accuracy: 86.69%\n",
      "Epoch [133/150], Training Loss: 0.0002\n",
      "Epoch [133/150], Test Loss: 0.6787, Test Accuracy: 86.69%\n",
      "Epoch [134/150], Training Loss: 0.0002\n",
      "Epoch [134/150], Test Loss: 0.6812, Test Accuracy: 86.69%\n",
      "Epoch [135/150], Training Loss: 0.0003\n",
      "Epoch [135/150], Test Loss: 0.6833, Test Accuracy: 86.27%\n",
      "Epoch [136/150], Training Loss: 0.0003\n",
      "Epoch [136/150], Test Loss: 0.6749, Test Accuracy: 86.27%\n",
      "Epoch [137/150], Training Loss: 0.0003\n",
      "Epoch [137/150], Test Loss: 0.6722, Test Accuracy: 86.69%\n",
      "Epoch [138/150], Training Loss: 0.0002\n",
      "Epoch [138/150], Test Loss: 0.6673, Test Accuracy: 86.58%\n",
      "Epoch [139/150], Training Loss: 0.0002\n",
      "Epoch [139/150], Test Loss: 0.6823, Test Accuracy: 86.48%\n",
      "Epoch [140/150], Training Loss: 0.0007\n",
      "Epoch [140/150], Test Loss: 0.6804, Test Accuracy: 86.69%\n",
      "Epoch [141/150], Training Loss: 0.0002\n",
      "Epoch [141/150], Test Loss: 0.6733, Test Accuracy: 86.90%\n",
      "Epoch [142/150], Training Loss: 0.0002\n",
      "Epoch [142/150], Test Loss: 0.6653, Test Accuracy: 86.37%\n",
      "Epoch [143/150], Training Loss: 0.0016\n",
      "Epoch [143/150], Test Loss: 0.6634, Test Accuracy: 86.58%\n",
      "Epoch [144/150], Training Loss: 0.0002\n",
      "Epoch [144/150], Test Loss: 0.6704, Test Accuracy: 86.58%\n",
      "Epoch [145/150], Training Loss: 0.0002\n",
      "Epoch [145/150], Test Loss: 0.6804, Test Accuracy: 86.16%\n",
      "Epoch [146/150], Training Loss: 0.0002\n",
      "Epoch [146/150], Test Loss: 0.6820, Test Accuracy: 86.79%\n",
      "Epoch [147/150], Training Loss: 0.0005\n",
      "Epoch [147/150], Test Loss: 0.6640, Test Accuracy: 87.21%\n",
      "Epoch [148/150], Training Loss: 0.0002\n",
      "Epoch [148/150], Test Loss: 0.6752, Test Accuracy: 86.37%\n",
      "Epoch [149/150], Training Loss: 0.0008\n",
      "Epoch [149/150], Test Loss: 0.6685, Test Accuracy: 86.69%\n",
      "Epoch [150/150], Training Loss: 0.0002\n",
      "Epoch [150/150], Test Loss: 0.6829, Test Accuracy: 87.21%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 150\n",
    "learning_rate = 0.01\n",
    "opt_eps = 1e-3\n",
    "clip_grad = 1.0\n",
    "device = 'cuda:1' \n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, eps=opt_eps)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=learning_rate,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Automatic Mixed Precision (AMP)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training and Testing Loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Move data to GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward and backward pass with AMP\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Log training loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Testing phase after each epoch\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Move data to GPU\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Log test accuracy and loss\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9b2bbc1-143e-42c2-881b-e002370749fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/j597s263/Models/Conv_Imagenette.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3445446-ae08-409e-bb88-8c1c71f0e489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
