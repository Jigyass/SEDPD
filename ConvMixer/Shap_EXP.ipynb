{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a673fe99-e73f-436d-b3a7-c652c112f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80a8d1c-96d1-44a4-8340-9a9abe6b0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "# ConvMixer model with hard-coded parameters\n",
    "def ConvMixer():\n",
    "    dim = 256          # Embedding dimension\n",
    "    depth = 8          # Number of ConvMixer blocks\n",
    "    kernel_size = 5    # Kernel size for depthwise convolution\n",
    "    patch_size = 4     # Patch size for initial convolution\n",
    "    n_classes = 10     # CIFAR-10 has 10 classes\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for _ in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b1118f-3742-4dc6-8458-72f3625e33d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load the entire model\n",
    "model = torch.load('/home/j597s263/scratch/j597s263/Models/Conv_Imagenette.mod', weights_only=False, map_location=\"cuda:0\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698a56cc-0bb6-4c41-98ab-88b009130d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack samples: 947\n",
      "Training samples: 7568\n",
      "Test samples: 954\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to ConvMixer input size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.Imagenette(root='/home/j597s263/scratch/j597s263/Datasets/imagenette', download=False, transform=transform)\n",
    "\n",
    "# Shuffle indices with a fixed random seed for reproducibility\n",
    "random.seed(42)  # Use any fixed seed for consistency\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split shuffled indices into training and testing\n",
    "train_indices = indices[:7568]\n",
    "test_indices = indices[7568:8522]\n",
    "attack_indices = indices[8522:]\n",
    "\n",
    "# Create Subsets\n",
    "train_data = Subset(dataset, train_indices)\n",
    "test_data = Subset(dataset, test_indices)\n",
    "attack_data = Subset(dataset, attack_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)  # Shuffle within batches\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)  # No shuffle for test set\n",
    "attack_loader = DataLoader(attack_data, batch_size=5, shuffle=True)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Attack samples: {len(attack_data)}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027b208d-79fa-4b95-97af-6520a261123f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport shap\\nimport torch\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n# Ensure the model is in evaluation mode\\nmodel.eval()\\n\\n# Select a small batch from the attack dataset to explain\\nattack_images, _ = next(iter(attack_loader))\\nattack_images = attack_images[:10]  # Select the first 10 images\\nattack_images = attack_images.to(device)\\n\\n# Select background samples from the training data\\nbackground_loader = DataLoader(train_data, batch_size=50, shuffle=True)\\nbackground_images, _ = next(iter(background_loader))  # Select the first batch\\nbackground_images = background_images.to(device)\\n\\n# Initialize SHAP Gradient Explainer\\nexplainer = shap.GradientExplainer(model, background_images)\\n\\n# Explain the attack images\\nshap_values = explainer.shap_values(attack_images)\\n\\n# Convert attack images to numpy format for visualization\\nattack_images_np = attack_images.cpu().numpy().transpose(0, 2, 3, 1)  # [N, H, W, C]\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Select a small batch from the attack dataset to explain\n",
    "attack_images, _ = next(iter(attack_loader))\n",
    "attack_images = attack_images[:10]  # Select the first 10 images\n",
    "attack_images = attack_images.to(device)\n",
    "\n",
    "# Select background samples from the training data\n",
    "background_loader = DataLoader(train_data, batch_size=50, shuffle=True)\n",
    "background_images, _ = next(iter(background_loader))  # Select the first batch\n",
    "background_images = background_images.to(device)\n",
    "\n",
    "# Initialize SHAP Gradient Explainer\n",
    "explainer = shap.GradientExplainer(model, background_images)\n",
    "\n",
    "# Explain the attack images\n",
    "shap_values = explainer.shap_values(attack_images)\n",
    "\n",
    "# Convert attack images to numpy format for visualization\n",
    "attack_images_np = attack_images.cpu().numpy().transpose(0, 2, 3, 1)  # [N, H, W, C]\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1cf9808-2a2c-45c9-a027-0dabe5232363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attack_images = attack_images[:10]  # Select the first 10 images\\nattack_images = attack_images.to(device)\\n\\n# Use a subset from the training data as the baseline\\nbaseline_images, _ = next(iter(train_loader))  # Take a batch from train_loader\\nbaseline_images = baseline_images[:1000]  # Limit to 50 images for efficiency\\nbaseline_images = baseline_images.to(device)\\n\\n# Initialize SHAP Gradient Explainer with the baseline\\nexplainer = shap.GradientExplainer(model, baseline_images)\\n\\n# Explain the attack images\\nshap_values = explainer.shap_values(attack_images)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Select a small batch from the attack dataset to explain\n",
    "attack_images, _ = next(iter(attack_loader))\n",
    "attack_images = attack_images[:10]  # Select the first 10 images\n",
    "attack_images = attack_images.to(device)\n",
    "\n",
    "# Use a subset from the training data as the baseline\n",
    "baseline_images, _ = next(iter(train_loader))  # Take a batch from train_loader\n",
    "baseline_images = baseline_images[:1000]  # Limit to 50 images for efficiency\n",
    "baseline_images = baseline_images.to(device)\n",
    "\n",
    "# Initialize SHAP Gradient Explainer with the baseline\n",
    "explainer = shap.GradientExplainer(model, baseline_images)\n",
    "\n",
    "# Explain the attack images\n",
    "shap_values = explainer.shap_values(attack_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b7fa6d9-bde6-4828-81f9-01b2c142f300",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.cuda.FloatTensor{[3, 224, 224]}, size=[224, 224]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m attack_image \u001b[38;5;241m=\u001b[39m attack_images[i]\n\u001b[1;32m     14\u001b[0m attack_image \u001b[38;5;241m=\u001b[39m attack_image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m shap_values \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattack_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 image processed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/shap/explainers/_gradient.py:158\u001b[0m, in \u001b[0;36mGradientExplainer.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, nsamples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, rseed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_variances\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the values for the model applied to X.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_variances\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/shap/explainers/_gradient.py:575\u001b[0m, in \u001b[0;36m_PyTorchGradient.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m X[a][j]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 575\u001b[0m \u001b[43msamples_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (t \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m t) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs[a][rind])\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39m\\\n\u001b[1;32m    576\u001b[0m     clone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     samples_delta[a][k] \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[a][rind])\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.cuda.FloatTensor{[3, 224, 224]}, size=[224, 224]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "attack_images, _ = next(iter(attack_loader))\n",
    "\n",
    "# Use a subset from the training data as the baseline\n",
    "baseline_images, _ = next(iter(train_loader))  # Take a batch from train_loader\n",
    "baseline_images = baseline_images[:1000]  # Limit to 50 images for efficiency\n",
    "baseline_images = baseline_images.to(device)\n",
    "\n",
    "explainer = shap.GradientExplainer(model, baseline_images)\n",
    "shap_values = np.zeros((3, 224, 224, 10))  # (Channels, Height, Width, Classes)\n",
    "\n",
    "for i in range(947):\n",
    "    attack_image = attack_images[i]\n",
    "    attack_image = attack_image.to(device)\n",
    "\n",
    "    shap_values += explainer.shap_values(attack_image)\n",
    "    print(\"1 image processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58524809-e230-4c90-bfe8-304ee78088ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 images processed in this batch\n",
      "5 images processed in this batch\n",
      "5 images processed in this batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m batch_images \u001b[38;5;241m=\u001b[39m batch_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Compute SHAP values for the batch\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m batch_shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (batch_size, 3, 224, 224, 10)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Sum the SHAP values into the aggregation array\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch_images)):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/shap/explainers/_gradient.py:158\u001b[0m, in \u001b[0;36mGradientExplainer.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, nsamples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, rseed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_variances\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the values for the model applied to X.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_variances\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/shap/explainers/_gradient.py:598\u001b[0m, in \u001b[0;36m_PyTorchGradient.shap_values\u001b[0;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, nsamples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m    597\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [samples_input[c][b:\u001b[38;5;28mmin\u001b[39m(b\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,nsamples)]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))]\n\u001b[0;32m--> 598\u001b[0m     grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    599\u001b[0m grad \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mconcatenate([g[z] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads], \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata))]\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/shap/explainers/_gradient.py:487\u001b[0m, in \u001b[0;36m_PyTorchGradient.gradient\u001b[0;34m(self, idx, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer\u001b[38;5;241m.\u001b[39mtarget_input\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(selected, x,\n\u001b[1;32m    488\u001b[0m                                  retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    489\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X)]\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grads\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/shap/explainers/_gradient.py:487\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer\u001b[38;5;241m.\u001b[39mtarget_input\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    489\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X)]\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grads\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "device='cuda'\n",
    "# Use a subset from the training data as the baseline\n",
    "baseline_images, _ = next(iter(train_loader))  # Take a batch from train_loader\n",
    "baseline_images = baseline_images[:1000]  # Limit to 50 images for efficiency\n",
    "baseline_images = baseline_images.to(device)\n",
    "\n",
    "# Initialize SHAP Gradient Explainer\n",
    "explainer = shap.GradientExplainer(model, baseline_images)\n",
    "\n",
    "# Initialize aggregation array for SHAP values\n",
    "shap_values = np.zeros((3, 224, 224, 10))  # (Channels, Height, Width, Classes)\n",
    "\n",
    "# Process attack images in batches\n",
    "for batch_images, _ in attack_loader:\n",
    "    batch_images = batch_images.to(device)\n",
    "\n",
    "    # Compute SHAP values for the batch\n",
    "    batch_shap_values = explainer.shap_values(batch_images)  # Shape: (batch_size, 3, 224, 224, 10)\n",
    "\n",
    "    # Sum the SHAP values into the aggregation array\n",
    "    for i in range(len(batch_images)):\n",
    "        shap_values += batch_shap_values[i]\n",
    "\n",
    "    print(f\"{len(batch_images)} images processed in this batch\")\n",
    "\n",
    "# Normalize SHAP values by the number of attack images\n",
    "shap_values /= len(attack_loader.dataset)\n",
    "\n",
    "print(\"SHAP value aggregation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae40bd8-a4ad-4206-ab52-6a0c15f08b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
