{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a4a5e-7152-41f9-b688-9660aac996ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e7dff9d-28a5-4307-8b0a-2714fc9ef68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.special import comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e70424d5-068e-45d2-91dc-3baaf9225f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 9469\n",
      "Training samples: 7568\n",
      "Test samples: 954\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to ConvMixer input size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.Imagenette(root='/home/j597s263/scratch/j597s263/Datasets/imagenette', download=False, transform=transform)\n",
    "\n",
    "# Shuffle indices with a fixed random seed for reproducibility\n",
    "random.seed(42)  # Use any fixed seed for consistency\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split shuffled indices into training and testing\n",
    "train_indices = indices[:7568]\n",
    "test_indices = indices[7568:8522]\n",
    "\n",
    "# Create Subsets\n",
    "train_data = Subset(dataset, train_indices)\n",
    "test_data = Subset(dataset, test_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)  # Shuffle within batches\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)  # No shuffle for test set\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "data_loader = train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3613e1a-e1d3-4b2e-a3c4-8739870fa3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def apply_samples_to_dataset(data_loader, sampled_rgb_values, pixel_coords, output_path):\n",
    "    \"\"\"\n",
    "    Apply sampled RGB values to images and save the dataset with labels to a file.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): DataLoader containing the images to modify.\n",
    "        sampled_rgb_values (dict): Dictionary of sampled RGB values for each pixel.\n",
    "        pixel_coords (list of tuples): List of pixel coordinates to evaluate.\n",
    "        output_path (str): Path to the file where the dataset will be saved.\n",
    "    \"\"\"\n",
    "    modified_images = []\n",
    "    labels = []\n",
    "\n",
    "    # Process each image in the data loader\n",
    "    for batch_idx, (images, batch_labels) in enumerate(data_loader):\n",
    "        images = images.clone()  # Clone to avoid modifying the original data\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        for img_idx in range(batch_size):\n",
    "            image_tensor = images[img_idx]\n",
    "            img_array = image_tensor.permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C)\n",
    "\n",
    "            height, width, _ = img_array.shape\n",
    "            for coord in pixel_coords:\n",
    "                x, y = coord\n",
    "                if x < height and y < width:\n",
    "                    if (x, y) in sampled_rgb_values:\n",
    "                        gray_levels = sampled_rgb_values[(x, y)]\n",
    "                        found = False\n",
    "                        for gray_level in gray_levels:\n",
    "                            if img_idx in gray_levels[gray_level]:\n",
    "                                found = True\n",
    "                                break\n",
    "                        if not found:\n",
    "                            img_array[x, y] = [0, 0, 0]  # Set to black if no match\n",
    "                    else:\n",
    "                        img_array[x, y] = [0, 0, 0]  # Set to black for unmatched coordinates\n",
    "\n",
    "            # Convert modified array back to tensor\n",
    "            modified_tensor = torch.from_numpy(img_array).permute(2, 0, 1)\n",
    "\n",
    "            # Add modified tensor and corresponding label to the dataset\n",
    "            modified_images.append(modified_tensor)\n",
    "            labels.append(batch_labels[img_idx].item())\n",
    "\n",
    "        print(f\"Processed batch {batch_idx + 1}/{len(data_loader)}\")\n",
    "\n",
    "    # Save the modified dataset\n",
    "    dataset = {\n",
    "        \"images\": torch.stack(modified_images),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "    torch.save(dataset, output_path)\n",
    "    print(f\"Modified dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1429bdac-2f5d-4fdc-b193-f034a3210ffb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pixel_coords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Function calls for each dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sampled_rgb_values \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/Sampled_Values/Img/t2e2.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m apply_samples_to_dataset(\n\u001b[1;32m      6\u001b[0m     data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[1;32m      7\u001b[0m     sampled_rgb_values\u001b[38;5;241m=\u001b[39msampled_rgb_values,\n\u001b[0;32m----> 8\u001b[0m     pixel_coords\u001b[38;5;241m=\u001b[39m\u001b[43mpixel_coords\u001b[49m,\n\u001b[1;32m      9\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/ConvImgE2.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m sampled_rgb_values \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/Sampled_Values/Img/t2e3.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     13\u001b[0m apply_samples_to_dataset(\n\u001b[1;32m     14\u001b[0m     data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[1;32m     15\u001b[0m     sampled_rgb_values\u001b[38;5;241m=\u001b[39msampled_rgb_values,\n\u001b[1;32m     16\u001b[0m     pixel_coords\u001b[38;5;241m=\u001b[39mpixel_coords,\n\u001b[1;32m     17\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/ConvImgE3.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pixel_coords' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Function calls for each dataset\n",
    "sampled_rgb_values = pickle.load(open(\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/Sampled_Values/Img/t2e2.pkl\", \"rb\"))\n",
    "apply_samples_to_dataset(\n",
    "    data_loader=data_loader,\n",
    "    sampled_rgb_values=sampled_rgb_values,\n",
    "    pixel_coords=pixel_coords,\n",
    "    output_path=\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/ConvImgE2.pt\"\n",
    ")\n",
    "\n",
    "sampled_rgb_values = pickle.load(open(\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/Sampled_Values/Img/t2e3.pkl\", \"rb\"))\n",
    "apply_samples_to_dataset(\n",
    "    data_loader=data_loader,\n",
    "    sampled_rgb_values=sampled_rgb_values,\n",
    "    pixel_coords=pixel_coords,\n",
    "    output_path=\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/ConvImgE3.pt\"\n",
    ")\n",
    "\n",
    "sampled_rgb_values = pickle.load(open(\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/Sampled_Values/Img/t2e4.pkl\", \"rb\"))\n",
    "apply_samples_to_dataset(\n",
    "    data_loader=data_loader,\n",
    "    sampled_rgb_values=sampled_rgb_values,\n",
    "    pixel_coords=pixel_coords,\n",
    "    output_path=\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/ConvImgE4.pt\"\n",
    ")\n",
    "\n",
    "sampled_rgb_values = pickle.load(open(\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/Sampled_Values/Img/t2e5.pkl\", \"rb\"))\n",
    "apply_samples_to_dataset(\n",
    "    data_loader=data_loader,\n",
    "    sampled_rgb_values=sampled_rgb_values,\n",
    "    pixel_coords=pixel_coords,\n",
    "    output_path=\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/ConvImgE5.pt\"\n",
    ")\n",
    "\n",
    "sampled_rgb_values = pickle.load(open(\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/Sampled_Values/Img/t2e6.pkl\", \"rb\"))\n",
    "apply_samples_to_dataset(\n",
    "    data_loader=data_loader,\n",
    "    sampled_rgb_values=sampled_rgb_values,\n",
    "    pixel_coords=pixel_coords,\n",
    "    output_path=\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/ConvImgE6.pt\"\n",
    ")\n",
    "\n",
    "sampled_rgb_values = pickle.load(open(\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/Sampled_Values/Img/t2e7.pkl\", \"rb\"))\n",
    "apply_samples_to_dataset(\n",
    "    data_loader=data_loader,\n",
    "    sampled_rgb_values=sampled_rgb_values,\n",
    "    pixel_coords=pixel_coords,\n",
    "    output_path=\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/ConvImgE7.pt\"\n",
    ")\n",
    "\n",
    "sampled_rgb_values = pickle.load(open(\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/Sampled_Values/Img/t2e8.pkl\", \"rb\"))\n",
    "apply_samples_to_dataset(\n",
    "    data_loader=data_loader,\n",
    "    sampled_rgb_values=sampled_rgb_values,\n",
    "    pixel_coords=pixel_coords,\n",
    "    output_path=\"/home/j597s263/scratch/j597s263/Datasets/Defense/Conv/ConvImgE8.pt\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
