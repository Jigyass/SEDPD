{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35928554-187b-452e-b4ea-ec5be33ecd29",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from time import time\n",
    "\n",
    "import config\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from classifier_models import PreActResNet18, ResNet18\n",
    "from networks.models import Denormalizer, NetC_MNIST, Normalizer\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import RandomErasing\n",
    "from utils.dataloader import PostTensorTransform, get_dataloader\n",
    "from utils.utils import progress_bar\n",
    "\n",
    "\n",
    "def get_model(opt):\n",
    "    netC = None\n",
    "    optimizerC = None\n",
    "    schedulerC = None\n",
    "\n",
    "    if opt.dataset == \"cifar10\" or opt.dataset == \"gtsrb\":\n",
    "        netC = PreActResNet18(num_classes=opt.num_classes).to(opt.device)\n",
    "    if opt.dataset == \"celeba\":\n",
    "        netC = ResNet18().to(opt.device)\n",
    "    if opt.dataset == \"mnist\":\n",
    "        netC = NetC_MNIST().to(opt.device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizerC = torch.optim.SGD(netC.parameters(), opt.lr_C, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    # Scheduler\n",
    "    schedulerC = torch.optim.lr_scheduler.MultiStepLR(optimizerC, opt.schedulerC_milestones, opt.schedulerC_lambda)\n",
    "\n",
    "    return netC, optimizerC, schedulerC\n",
    "\n",
    "\n",
    "def train(netC, optimizerC, schedulerC, train_dl, noise_grid, identity_grid, tf_writer, epoch, opt):\n",
    "    print(\" Train:\")\n",
    "    netC.train()\n",
    "    rate_bd = opt.pc\n",
    "    total_loss_ce = 0\n",
    "    total_sample = 0\n",
    "\n",
    "    total_clean = 0\n",
    "    total_bd = 0\n",
    "    total_cross = 0\n",
    "    total_clean_correct = 0\n",
    "    total_bd_correct = 0\n",
    "    total_cross_correct = 0\n",
    "    criterion_CE = torch.nn.CrossEntropyLoss()\n",
    "    criterion_BCE = torch.nn.BCELoss()\n",
    "\n",
    "    denormalizer = Denormalizer(opt)\n",
    "    transforms = PostTensorTransform(opt).to(opt.device)\n",
    "    total_time = 0\n",
    "\n",
    "    avg_acc_cross = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dl):\n",
    "        optimizerC.zero_grad()\n",
    "\n",
    "        inputs, targets = inputs.to(opt.device), targets.to(opt.device)\n",
    "        bs = inputs.shape[0]\n",
    "\n",
    "        # Create backdoor data\n",
    "        num_bd = int(bs * rate_bd)\n",
    "        num_cross = int(num_bd * opt.cross_ratio)\n",
    "        grid_temps = (identity_grid + opt.s * noise_grid / opt.input_height) * opt.grid_rescale\n",
    "        grid_temps = torch.clamp(grid_temps, -1, 1)\n",
    "\n",
    "        ins = torch.rand(num_cross, opt.input_height, opt.input_height, 2).to(opt.device) * 2 - 1\n",
    "        grid_temps2 = grid_temps.repeat(num_cross, 1, 1, 1) + ins / opt.input_height\n",
    "        grid_temps2 = torch.clamp(grid_temps2, -1, 1)\n",
    "\n",
    "        inputs_bd = F.grid_sample(inputs[:num_bd], grid_temps.repeat(num_bd, 1, 1, 1), align_corners=True)\n",
    "        if opt.attack_mode == \"all2one\":\n",
    "            targets_bd = torch.ones_like(targets[:num_bd]) * opt.target_label\n",
    "        if opt.attack_mode == \"all2all\":\n",
    "            targets_bd = torch.remainder(targets[:num_bd] + 1, opt.num_classes)\n",
    "\n",
    "        inputs_cross = F.grid_sample(inputs[num_bd : (num_bd + num_cross)], grid_temps2, align_corners=True)\n",
    "\n",
    "        total_inputs = torch.cat([inputs_bd, inputs_cross, inputs[(num_bd + num_cross) :]], dim=0)\n",
    "        total_inputs = transforms(total_inputs)\n",
    "        total_targets = torch.cat([targets_bd, targets[num_bd:]], dim=0)\n",
    "        start = time()\n",
    "        total_preds = netC(total_inputs)\n",
    "        total_time += time() - start\n",
    "\n",
    "        loss_ce = criterion_CE(total_preds, total_targets)\n",
    "\n",
    "        loss = loss_ce\n",
    "        loss.backward()\n",
    "\n",
    "        optimizerC.step()\n",
    "\n",
    "        total_sample += bs\n",
    "        total_loss_ce += loss_ce.detach()\n",
    "\n",
    "        total_clean += bs - num_bd - num_cross\n",
    "        total_bd += num_bd\n",
    "        total_cross += num_cross\n",
    "        total_clean_correct += torch.sum(\n",
    "            torch.argmax(total_preds[(num_bd + num_cross) :], dim=1) == total_targets[(num_bd + num_cross) :]\n",
    "        )\n",
    "        total_bd_correct += torch.sum(torch.argmax(total_preds[:num_bd], dim=1) == targets_bd)\n",
    "        if num_cross:\n",
    "            total_cross_correct += torch.sum(\n",
    "                torch.argmax(total_preds[num_bd : (num_bd + num_cross)], dim=1)\n",
    "                == total_targets[num_bd : (num_bd + num_cross)]\n",
    "            )\n",
    "            avg_acc_cross = total_cross_correct * 100.0 / total_cross\n",
    "\n",
    "        avg_acc_clean = total_clean_correct * 100.0 / total_clean\n",
    "        avg_acc_bd = total_bd_correct * 100.0 / total_bd\n",
    "\n",
    "        avg_loss_ce = total_loss_ce / total_sample\n",
    "\n",
    "        if num_cross:\n",
    "            progress_bar(\n",
    "                batch_idx,\n",
    "                len(train_dl),\n",
    "                \"CE Loss: {:.4f} | Clean Acc: {:.4f} | Bd Acc: {:.4f} | Cross Acc: {:.4f}\".format(\n",
    "                    avg_loss_ce, avg_acc_clean, avg_acc_bd, avg_acc_cross\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            progress_bar(\n",
    "                batch_idx,\n",
    "                len(train_dl),\n",
    "                \"CE Loss: {:.4f} | Clean Acc: {:.4f} | Bd Acc: {:.4f} \".format(avg_loss_ce, avg_acc_clean, avg_acc_bd),\n",
    "            )\n",
    "\n",
    "        # Save image for debugging\n",
    "        if not batch_idx % 50:\n",
    "            if not os.path.exists(opt.temps):\n",
    "                os.makedirs(opt.temps)\n",
    "            path = os.path.join(opt.temps, \"backdoor_image.png\")\n",
    "            torchvision.utils.save_image(inputs_bd, path, normalize=True)\n",
    "\n",
    "        # Image for tensorboard\n",
    "        if batch_idx == len(train_dl) - 2:\n",
    "            residual = inputs_bd - inputs[:num_bd]\n",
    "            batch_img = torch.cat([inputs[:num_bd], inputs_bd, total_inputs[:num_bd], residual], dim=2)\n",
    "            batch_img = denormalizer(batch_img)\n",
    "            batch_img = F.upsample(batch_img, scale_factor=(4, 4))\n",
    "            grid = torchvision.utils.make_grid(batch_img, normalize=True)\n",
    "\n",
    "    # for tensorboard\n",
    "    if not epoch % 1:\n",
    "        tf_writer.add_scalars(\n",
    "            \"Clean Accuracy\", {\"Clean\": avg_acc_clean, \"Bd\": avg_acc_bd, \"Cross\": avg_acc_cross}, epoch\n",
    "        )\n",
    "        tf_writer.add_image(\"Images\", grid, global_step=epoch)\n",
    "\n",
    "    schedulerC.step()\n",
    "\n",
    "\n",
    "def eval(\n",
    "    netC,\n",
    "    optimizerC,\n",
    "    schedulerC,\n",
    "    test_dl,\n",
    "    noise_grid,\n",
    "    identity_grid,\n",
    "    best_clean_acc,\n",
    "    best_bd_acc,\n",
    "    best_cross_acc,\n",
    "    tf_writer,\n",
    "    epoch,\n",
    "    opt,\n",
    "):\n",
    "    print(\" Eval:\")\n",
    "    netC.eval()\n",
    "\n",
    "    total_sample = 0\n",
    "    total_clean_correct = 0\n",
    "    total_bd_correct = 0\n",
    "    total_cross_correct = 0\n",
    "    total_ae_loss = 0\n",
    "\n",
    "    criterion_BCE = torch.nn.BCELoss()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_dl):\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = inputs.to(opt.device), targets.to(opt.device)\n",
    "            bs = inputs.shape[0]\n",
    "            total_sample += bs\n",
    "\n",
    "            # Evaluate Clean\n",
    "            preds_clean = netC(inputs)\n",
    "            total_clean_correct += torch.sum(torch.argmax(preds_clean, 1) == targets)\n",
    "\n",
    "            # Evaluate Backdoor\n",
    "            grid_temps = (identity_grid + opt.s * noise_grid / opt.input_height) * opt.grid_rescale\n",
    "            grid_temps = torch.clamp(grid_temps, -1, 1)\n",
    "\n",
    "            ins = torch.rand(bs, opt.input_height, opt.input_height, 2).to(opt.device) * 2 - 1\n",
    "            grid_temps2 = grid_temps.repeat(bs, 1, 1, 1) + ins / opt.input_height\n",
    "            grid_temps2 = torch.clamp(grid_temps2, -1, 1)\n",
    "\n",
    "            inputs_bd = F.grid_sample(inputs, grid_temps.repeat(bs, 1, 1, 1), align_corners=True)\n",
    "            if opt.attack_mode == \"all2one\":\n",
    "                targets_bd = torch.ones_like(targets) * opt.target_label\n",
    "            if opt.attack_mode == \"all2all\":\n",
    "                targets_bd = torch.remainder(targets + 1, opt.num_classes)\n",
    "            preds_bd = netC(inputs_bd)\n",
    "            total_bd_correct += torch.sum(torch.argmax(preds_bd, 1) == targets_bd)\n",
    "\n",
    "            acc_clean = total_clean_correct * 100.0 / total_sample\n",
    "            acc_bd = total_bd_correct * 100.0 / total_sample\n",
    "\n",
    "            # Evaluate cross\n",
    "            if opt.cross_ratio:\n",
    "                inputs_cross = F.grid_sample(inputs, grid_temps2, align_corners=True)\n",
    "                preds_cross = netC(inputs_cross)\n",
    "                total_cross_correct += torch.sum(torch.argmax(preds_cross, 1) == targets)\n",
    "\n",
    "                acc_cross = total_cross_correct * 100.0 / total_sample\n",
    "\n",
    "                info_string = (\n",
    "                    \"Clean Acc: {:.4f} - Best: {:.4f} | Bd Acc: {:.4f} - Best: {:.4f} | Cross: {:.4f}\".format(\n",
    "                        acc_clean, best_clean_acc, acc_bd, best_bd_acc, acc_cross, best_cross_acc\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                info_string = \"Clean Acc: {:.4f} - Best: {:.4f} | Bd Acc: {:.4f} - Best: {:.4f}\".format(\n",
    "                    acc_clean, best_clean_acc, acc_bd, best_bd_acc\n",
    "                )\n",
    "            progress_bar(batch_idx, len(test_dl), info_string)\n",
    "\n",
    "    # tensorboard\n",
    "    if not epoch % 1:\n",
    "        tf_writer.add_scalars(\"Test Accuracy\", {\"Clean\": acc_clean, \"Bd\": acc_bd}, epoch)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if acc_clean > best_clean_acc or (acc_clean > best_clean_acc - 0.1 and acc_bd > best_bd_acc):\n",
    "        print(\" Saving...\")\n",
    "        best_clean_acc = acc_clean\n",
    "        best_bd_acc = acc_bd\n",
    "        if opt.cross_ratio:\n",
    "            best_cross_acc = acc_cross\n",
    "        else:\n",
    "            best_cross_acc = torch.tensor([0])\n",
    "        state_dict = {\n",
    "            \"netC\": netC.state_dict(),\n",
    "            \"schedulerC\": schedulerC.state_dict(),\n",
    "            \"optimizerC\": optimizerC.state_dict(),\n",
    "            \"best_clean_acc\": best_clean_acc,\n",
    "            \"best_bd_acc\": best_bd_acc,\n",
    "            \"best_cross_acc\": best_cross_acc,\n",
    "            \"epoch_current\": epoch,\n",
    "            \"identity_grid\": identity_grid,\n",
    "            \"noise_grid\": noise_grid,\n",
    "        }\n",
    "        torch.save(state_dict, opt.ckpt_path)\n",
    "        with open(os.path.join(opt.ckpt_folder, \"results.txt\"), \"w+\") as f:\n",
    "            results_dict = {\n",
    "                \"clean_acc\": best_clean_acc.item(),\n",
    "                \"bd_acc\": best_bd_acc.item(),\n",
    "                \"cross_acc\": best_cross_acc.item(),\n",
    "            }\n",
    "            json.dump(results_dict, f, indent=2)\n",
    "\n",
    "    return best_clean_acc, best_bd_acc, best_cross_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "    opt = config.get_arguments().parse_args()\n",
    "\n",
    "    if opt.dataset in [\"mnist\", \"cifar10\"]:\n",
    "        opt.num_classes = 10\n",
    "    elif opt.dataset == \"gtsrb\":\n",
    "        opt.num_classes = 43\n",
    "    elif opt.dataset == \"celeba\":\n",
    "        opt.num_classes = 8\n",
    "    else:\n",
    "        raise Exception(\"Invalid Dataset\")\n",
    "\n",
    "    if opt.dataset == \"cifar10\":\n",
    "        opt.input_height = 32\n",
    "        opt.input_width = 32\n",
    "        opt.input_channel = 3\n",
    "    elif opt.dataset == \"gtsrb\":\n",
    "        opt.input_height = 32\n",
    "        opt.input_width = 32\n",
    "        opt.input_channel = 3\n",
    "    elif opt.dataset == \"mnist\":\n",
    "        opt.input_height = 28\n",
    "        opt.input_width = 28\n",
    "        opt.input_channel = 1\n",
    "    elif opt.dataset == \"celeba\":\n",
    "        opt.input_height = 64\n",
    "        opt.input_width = 64\n",
    "        opt.input_channel = 3\n",
    "    else:\n",
    "        raise Exception(\"Invalid Dataset\")\n",
    "\n",
    "    # Dataset\n",
    "    train_dl = get_dataloader(opt, True)\n",
    "    test_dl = get_dataloader(opt, False)\n",
    "\n",
    "    # prepare model\n",
    "    netC, optimizerC, schedulerC = get_model(opt)\n",
    "\n",
    "    # Load pretrained model\n",
    "    mode = opt.attack_mode\n",
    "    opt.ckpt_folder = os.path.join(opt.checkpoints, opt.dataset)\n",
    "    opt.ckpt_path = os.path.join(opt.ckpt_folder, \"{}_{}_morph.pth.tar\".format(opt.dataset, mode))\n",
    "    opt.log_dir = os.path.join(opt.ckpt_folder, \"log_dir\")\n",
    "    if not os.path.exists(opt.log_dir):\n",
    "        os.makedirs(opt.log_dir)\n",
    "\n",
    "    if opt.continue_training:\n",
    "        if os.path.exists(opt.ckpt_path):\n",
    "            print(\"Continue training!!\")\n",
    "            state_dict = torch.load(opt.ckpt_path)\n",
    "            netC.load_state_dict(state_dict[\"netC\"])\n",
    "            optimizerC.load_state_dict(state_dict[\"optimizerC\"])\n",
    "            schedulerC.load_state_dict(state_dict[\"schedulerC\"])\n",
    "            best_clean_acc = state_dict[\"best_clean_acc\"]\n",
    "            best_bd_acc = state_dict[\"best_bd_acc\"]\n",
    "            best_cross_acc = state_dict[\"best_cross_acc\"]\n",
    "            epoch_current = state_dict[\"epoch_current\"]\n",
    "            identity_grid = state_dict[\"identity_grid\"]\n",
    "            noise_grid = state_dict[\"noise_grid\"]\n",
    "            tf_writer = SummaryWriter(log_dir=opt.log_dir)\n",
    "        else:\n",
    "            print(\"Pretrained model doesnt exist\")\n",
    "            exit()\n",
    "    else:\n",
    "        print(\"Train from scratch!!!\")\n",
    "        best_clean_acc = 0.0\n",
    "        best_bd_acc = 0.0\n",
    "        best_cross_acc = 0.0\n",
    "        epoch_current = 0\n",
    "\n",
    "        # Prepare grid\n",
    "        ins = torch.rand(1, 2, opt.k, opt.k) * 2 - 1\n",
    "        ins = ins / torch.mean(torch.abs(ins))\n",
    "        noise_grid = (\n",
    "            F.upsample(ins, size=opt.input_height, mode=\"bicubic\", align_corners=True)\n",
    "            .permute(0, 2, 3, 1)\n",
    "            .to(opt.device)\n",
    "        )\n",
    "        array1d = torch.linspace(-1, 1, steps=opt.input_height)\n",
    "        x, y = torch.meshgrid(array1d, array1d)\n",
    "        identity_grid = torch.stack((y, x), 2)[None, ...].to(opt.device)\n",
    "\n",
    "        shutil.rmtree(opt.ckpt_folder, ignore_errors=True)\n",
    "        os.makedirs(opt.log_dir)\n",
    "        with open(os.path.join(opt.ckpt_folder, \"opt.json\"), \"w+\") as f:\n",
    "            json.dump(opt.__dict__, f, indent=2)\n",
    "        tf_writer = SummaryWriter(log_dir=opt.log_dir)\n",
    "\n",
    "    for epoch in range(epoch_current, opt.n_iters):\n",
    "        print(\"Epoch {}:\".format(epoch + 1))\n",
    "        train(netC, optimizerC, schedulerC, train_dl, noise_grid, identity_grid, tf_writer, epoch, opt)\n",
    "        best_clean_acc, best_bd_acc, best_cross_acc = eval(\n",
    "            netC,\n",
    "            optimizerC,\n",
    "            schedulerC,\n",
    "            test_dl,\n",
    "            noise_grid,\n",
    "            identity_grid,\n",
    "            best_clean_acc,\n",
    "            best_bd_acc,\n",
    "            best_cross_acc,\n",
    "            tf_writer,\n",
    "            epoch,\n",
    "            opt,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb4218a-894c-46a7-adbc-ee6084821280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training samples: 50000\n",
      "Training samples after split: 45000\n",
      "Attack samples: 5000\n",
      "Testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='/home/j597s263/scratch/j597s263/Datasets/cifar10', \n",
    "                                 download=False, \n",
    "                                 transform=transform, \n",
    "                                 train=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='/home/j597s263/scratch/j597s263/Datasets/cifar10', \n",
    "                                download=False, \n",
    "                                transform=transform, \n",
    "                                train=False)\n",
    "\n",
    "random.seed(42)  \n",
    "train_indices = list(range(len(train_dataset)))\n",
    "random.shuffle(train_indices)\n",
    "\n",
    "split_idx = int(0.9 * len(train_indices))  \n",
    "train_indices, attack_indices = train_indices[:split_idx], train_indices[split_idx:]\n",
    "\n",
    "train_data = Subset(train_dataset, train_indices)\n",
    "attack_data = Subset(train_dataset, attack_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)  \n",
    "attack_loader = DataLoader(attack_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Original training samples: {len(train_dataset)}\")\n",
    "print(f\"Training samples after split: {len(train_data)}\")\n",
    "print(f\"Attack samples: {len(attack_data)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f7b675-cdfc-4011-ae7d-886ff2a3b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def create_all2one_backdoor_dataset(data_loader, target_label, save_path, opt):\n",
    "    \"\"\"\n",
    "    Creates a backdoor dataset with an all2one attack.\n",
    "\n",
    "    Args:\n",
    "        data_loader: PyTorch DataLoader object with the original dataset.\n",
    "        target_label: Target label for all poisoned samples.\n",
    "        save_path: Path to save the poisoned dataset.\n",
    "        opt: Options object containing dataset parameters like input size and poison rate.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    identity_grid = generate_identity_grid(opt.input_height, opt.input_width, opt.device)\n",
    "    noise_grid = generate_noise_grid(opt.input_height, opt.input_width, opt.device, opt.s)\n",
    "\n",
    "    poisoned_data = []\n",
    "    poisoned_labels = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(data_loader, desc=\"Generating Backdoor Dataset\")):\n",
    "        inputs, targets = inputs.to(opt.device), targets.to(opt.device)\n",
    "\n",
    "        # Poisoning fraction\n",
    "        bs = inputs.size(0)\n",
    "        num_poisoned = int(bs * opt.poison_rate)\n",
    "\n",
    "        # Generate poisoned samples\n",
    "        if num_poisoned > 0:\n",
    "            grid_temps = (identity_grid + opt.s * noise_grid / opt.input_height) * opt.grid_rescale\n",
    "            grid_temps = torch.clamp(grid_temps, -1, 1)\n",
    "\n",
    "            poisoned_inputs = F.grid_sample(inputs[:num_poisoned], grid_temps.repeat(num_poisoned, 1, 1, 1), align_corners=True)\n",
    "            poisoned_targets = torch.ones(num_poisoned, dtype=torch.long, device=opt.device) * target_label\n",
    "\n",
    "            # Combine poisoned and clean samples\n",
    "            inputs = torch.cat([poisoned_inputs, inputs[num_poisoned:]], dim=0)\n",
    "            targets = torch.cat([poisoned_targets, targets[num_poisoned:]], dim=0)\n",
    "\n",
    "        poisoned_data.append(inputs.cpu())\n",
    "        poisoned_labels.append(targets.cpu())\n",
    "\n",
    "    # Save poisoned dataset\n",
    "    poisoned_data = torch.cat(poisoned_data, dim=0)\n",
    "    poisoned_labels = torch.cat(poisoned_labels, dim=0)\n",
    "\n",
    "    torch.save((poisoned_data, poisoned_labels), os.path.join(save_path, \"poisoned_dataset.pth\"))\n",
    "    print(f\"Poisoned dataset saved to {save_path}/poisoned_dataset.pth\")\n",
    "\n",
    "def generate_identity_grid(height, width, device):\n",
    "    \"\"\"Generates an identity grid.\"\"\"\n",
    "    array1d = torch.linspace(-1, 1, steps=height)\n",
    "    x, y = torch.meshgrid(array1d, array1d, indexing=\"ij\")\n",
    "    return torch.stack((y, x), 2)[None, ...].to(device)\n",
    "\n",
    "def generate_noise_grid(height, width, device, scale):\n",
    "    \"\"\"Generates a random noise grid.\"\"\"\n",
    "    ins = torch.rand(1, 2, height // 2, width // 2, device=device) * 2 - 1\n",
    "    ins = ins / torch.mean(torch.abs(ins))\n",
    "    return F.upsample(ins, size=(height, width), mode=\"bicubic\", align_corners=True).permute(0, 2, 3, 1)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    class Options:\n",
    "        input_height = 32\n",
    "        input_width = 32\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        poison_rate = 0.1\n",
    "        s = 0.2\n",
    "        grid_rescale = 1.0\n",
    "\n",
    "    opt = Options()\n",
    "\n",
    "    # Target label for backdoor attack\n",
    "    target_label = 4\n",
    "\n",
    "    # Save path for poisoned dataset\n",
    "    save_path = \"./poisoned_data\"\n",
    "\n",
    "    create_all2one_backdoor_dataset(data_loader, target_label, save_path, opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
