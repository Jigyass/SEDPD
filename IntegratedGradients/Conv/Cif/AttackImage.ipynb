{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb6c82b6-3436-47b8-927e-372debd8fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "# ConvMixer model with hard-coded parameters\n",
    "def ConvMixer():\n",
    "    dim = 256          # Embedding dimension\n",
    "    depth = 8          # Number of ConvMixer blocks\n",
    "    kernel_size = 5    # Kernel size for depthwise convolution\n",
    "    patch_size = 4     # Patch size for initial convolution\n",
    "    n_classes = 10     # CIFAR-10 has 10 classes\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for _ in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8011eda0-ebce-45b5-b372-ad87bc4dcf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "import torch\n",
    "\n",
    "# Define the path to the model\n",
    "device = \"cuda\" \n",
    "\n",
    "# Load the model\n",
    "model = torch.load('/home/j597s263/scratch/j597s263/Models/ConvModels/Base/ConvMNIBase.mod', weights_only=False, map_location=\"cuda\")\n",
    "model = model.to(device)\n",
    "model.eval()  \n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920d96b3-c50e-4ac0-8568-06d9718656e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 60000\n",
      "Training samples after split: 54000\n",
      "Attack samples: 6000\n",
      "Testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "# Define dataset root directory\n",
    "mnist_root = '/home/j597s263/scratch/j597s263/Datasets/MNIST'\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=mnist_root, transform=transform, train=True, download=True)\n",
    "test_dataset = datasets.MNIST(root=mnist_root, transform=transform, train=False, download=True)\n",
    "\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "random.shuffle(train_indices)  \n",
    "\n",
    "split_idx = int(0.9 * len(train_indices))  \n",
    "train_indices, attack_indices = train_indices[:split_idx], train_indices[split_idx:]\n",
    "\n",
    "train_data = Subset(train_dataset, train_indices)\n",
    "attack_data = Subset(train_dataset, attack_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)  # Shuffle within batches\n",
    "attack_loader = DataLoader(attack_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Training samples after split: {len(train_data)}\")\n",
    "print(f\"Attack samples: {len(attack_data)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05264b6f-9032-4c78-84df-27e4a58faba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mIG_ConvImg.npy\u001b[0m/    lime_ConvImag.npy   Shap_Imagenette.npy\n",
      " \u001b[01;34mIG_ConvMNI\u001b[0m/        lime_ConvMNI.npy   'TopPixels(Shap_ConvImag).npy'\n",
      " \u001b[01;34mIG_ConvMNI.npy\u001b[0m/    ShapCifarConv.npy\n",
      " lime_ConvCif.npy   SHAP_ConvMNI.npy\n"
     ]
    }
   ],
   "source": [
    "ls /home/j597s263/scratch/j597s263/Datasets/Explanation_values/Conv/IG_ConvMNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a8bf47-e6f7-42f8-8a20-6379e66a18fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image 1/94\n",
      "Processed image 2/94\n",
      "Processed image 3/94\n",
      "Processed image 4/94\n",
      "Processed image 5/94\n",
      "Processed image 6/94\n",
      "Processed image 7/94\n",
      "Processed image 8/94\n",
      "Processed image 9/94\n",
      "Processed image 10/94\n",
      "Processed image 11/94\n",
      "Processed image 12/94\n",
      "Processed image 13/94\n",
      "Processed image 14/94\n",
      "Processed image 15/94\n",
      "Processed image 16/94\n",
      "Processed image 17/94\n",
      "Processed image 18/94\n",
      "Processed image 19/94\n",
      "Processed image 20/94\n",
      "Processed image 21/94\n",
      "Processed image 22/94\n",
      "Processed image 23/94\n",
      "Processed image 24/94\n",
      "Processed image 25/94\n",
      "Processed image 26/94\n",
      "Processed image 27/94\n",
      "Processed image 28/94\n",
      "Processed image 29/94\n",
      "Processed image 30/94\n",
      "Processed image 31/94\n",
      "Processed image 32/94\n",
      "Processed image 33/94\n",
      "Processed image 34/94\n",
      "Processed image 35/94\n",
      "Processed image 36/94\n",
      "Processed image 37/94\n",
      "Processed image 38/94\n",
      "Processed image 39/94\n",
      "Processed image 40/94\n",
      "Processed image 41/94\n",
      "Processed image 42/94\n",
      "Processed image 43/94\n",
      "Processed image 44/94\n",
      "Processed image 45/94\n",
      "Processed image 46/94\n",
      "Processed image 47/94\n",
      "Processed image 48/94\n",
      "Processed image 49/94\n",
      "Processed image 50/94\n",
      "Processed image 51/94\n",
      "Processed image 52/94\n",
      "Processed image 53/94\n",
      "Processed image 54/94\n",
      "Processed image 55/94\n",
      "Processed image 56/94\n",
      "Processed image 57/94\n",
      "Processed image 58/94\n",
      "Processed image 59/94\n",
      "Processed image 60/94\n",
      "Processed image 61/94\n",
      "Processed image 62/94\n",
      "Processed image 63/94\n",
      "Processed image 64/94\n",
      "Processed image 65/94\n",
      "Processed image 66/94\n",
      "Processed image 67/94\n",
      "Processed image 68/94\n",
      "Processed image 69/94\n",
      "Processed image 70/94\n",
      "Processed image 71/94\n",
      "Processed image 72/94\n",
      "Processed image 73/94\n",
      "Processed image 74/94\n",
      "Processed image 75/94\n",
      "Processed image 76/94\n",
      "Processed image 77/94\n",
      "Processed image 78/94\n",
      "Processed image 79/94\n",
      "Processed image 80/94\n",
      "Processed image 81/94\n",
      "Processed image 82/94\n",
      "Processed image 83/94\n",
      "Processed image 84/94\n",
      "Processed image 85/94\n",
      "Processed image 86/94\n",
      "Processed image 87/94\n",
      "Processed image 88/94\n",
      "Processed image 89/94\n",
      "Processed image 90/94\n",
      "Processed image 91/94\n",
      "Processed image 92/94\n",
      "Processed image 93/94\n",
      "Processed image 94/94\n",
      "[[417. 417. 417. ... 417. 417. 417.]\n",
      " [417. 417. 417. ... 417. 417. 417.]\n",
      " [417. 417. 417. ... 417. 417. 417.]\n",
      " ...\n",
      " [417. 417. 417. ... 417. 417. 417.]\n",
      " [417. 417. 417. ... 417. 417. 417.]\n",
      " [417. 417. 417. ... 417. 417. 417.]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Initialize the aggregated array for MNIST (single channel)\n",
    "aggregated_explanations = np.zeros((224, 224), dtype=np.float32)\n",
    "\n",
    "# Define the directory containing the explanations\n",
    "explanations_dir = \"/home/j597s263/scratch/j597s263/Datasets/Explanation_values/Conv/IG_ConvMNI\"\n",
    "\n",
    "# Iterate through the attack loader to align images and their explanations\n",
    "for idx, (images, labels) in enumerate(attack_loader):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    predicted_labels = outputs.argmax(dim=1).tolist()  # Convert tensor to list\n",
    "    true_labels = labels.tolist()  # Convert tensor to list\n",
    "\n",
    "    # Load explanation file\n",
    "    explanation_file = os.path.join(explanations_dir, f\"explanation_{idx}.npy\")\n",
    "    explanation_with_label = np.load(explanation_file)  # Shape expected: (1, 224, 224) for MNIST\n",
    "\n",
    "    # Ensure explanation is grayscale (1-channel)\n",
    "    explanation = explanation_with_label[0]  # Shape: (224, 224) after removing channel\n",
    "\n",
    "    # Accumulate explanations\n",
    "    aggregated_explanations += explanation  # Direct sum for MNIST (no RGB channels)\n",
    "\n",
    "    print(f\"Processed image {idx + 1}/{len(attack_loader)}\")\n",
    "\n",
    "print(aggregated_explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a2da89-b136-49b8-ac50-2d0f8f0868a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 22 Pixel Locations and Values:\n",
      "Pixel (np.int64(0), np.int64(39)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(38)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(37)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(36)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(35)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(34)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(33)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(32)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(47)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(46)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(45)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(44)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(43)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(42)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(41)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(40)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(55)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(54)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(53)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(52)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(51)): Value 417.0000\n",
      "Pixel (np.int64(0), np.int64(50)): Value 417.0000\n"
     ]
    }
   ],
   "source": [
    "flattened_indices = aggregated_explanations.flatten().argsort()[-22:][::-1]  # Indices of top 22 values\n",
    "\n",
    "top_22_coords = np.unravel_index(flattened_indices, aggregated_explanations.shape)\n",
    "top_22_coords = list(zip(top_22_coords[0], top_22_coords[1]))\n",
    "\n",
    "top_22_values = [aggregated_explanations[x, y] for x, y in top_22_coords]\n",
    "\n",
    "top_22_pixels = list(zip(top_22_coords, top_22_values))\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 22 Pixel Locations and Values:\")\n",
    "for coord, value in top_22_pixels:\n",
    "    print(f\"Pixel {coord}: Value {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8518284-426b-45dd-ae1a-70c288798fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/j597s263/scratch/j597s263/Datasets/Attack/ConvIGMni': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls /home/j597s263/scratch/j597s263/Datasets/Attack/ConvIGMni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3676a68b-12ad-49ca-8ebf-b979aeb4a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved modified image 1/94\n",
      "Saved modified image 2/94\n",
      "Saved modified image 3/94\n",
      "Saved modified image 4/94\n",
      "Saved modified image 5/94\n",
      "Saved modified image 6/94\n",
      "Saved modified image 7/94\n",
      "Saved modified image 8/94\n",
      "Saved modified image 9/94\n",
      "Saved modified image 10/94\n",
      "Saved modified image 11/94\n",
      "Saved modified image 12/94\n",
      "Saved modified image 13/94\n",
      "Saved modified image 14/94\n",
      "Saved modified image 15/94\n",
      "Saved modified image 16/94\n",
      "Saved modified image 17/94\n",
      "Saved modified image 18/94\n",
      "Saved modified image 19/94\n",
      "Saved modified image 20/94\n",
      "Saved modified image 21/94\n",
      "Saved modified image 22/94\n",
      "Saved modified image 23/94\n",
      "Saved modified image 24/94\n",
      "Saved modified image 25/94\n",
      "Saved modified image 26/94\n",
      "Saved modified image 27/94\n",
      "Saved modified image 28/94\n",
      "Saved modified image 29/94\n",
      "Saved modified image 30/94\n",
      "Saved modified image 31/94\n",
      "Saved modified image 32/94\n",
      "Saved modified image 33/94\n",
      "Saved modified image 34/94\n",
      "Saved modified image 35/94\n",
      "Saved modified image 36/94\n",
      "Saved modified image 37/94\n",
      "Saved modified image 38/94\n",
      "Saved modified image 39/94\n",
      "Saved modified image 40/94\n",
      "Saved modified image 41/94\n",
      "Saved modified image 42/94\n",
      "Saved modified image 43/94\n",
      "Saved modified image 44/94\n",
      "Saved modified image 45/94\n",
      "Saved modified image 46/94\n",
      "Saved modified image 47/94\n",
      "Saved modified image 48/94\n",
      "Saved modified image 49/94\n",
      "Saved modified image 50/94\n",
      "Saved modified image 51/94\n",
      "Saved modified image 52/94\n",
      "Saved modified image 53/94\n",
      "Saved modified image 54/94\n",
      "Saved modified image 55/94\n",
      "Saved modified image 56/94\n",
      "Saved modified image 57/94\n",
      "Saved modified image 58/94\n",
      "Saved modified image 59/94\n",
      "Saved modified image 60/94\n",
      "Saved modified image 61/94\n",
      "Saved modified image 62/94\n",
      "Saved modified image 63/94\n",
      "Saved modified image 64/94\n",
      "Saved modified image 65/94\n",
      "Saved modified image 66/94\n",
      "Saved modified image 67/94\n",
      "Saved modified image 68/94\n",
      "Saved modified image 69/94\n",
      "Saved modified image 70/94\n",
      "Saved modified image 71/94\n",
      "Saved modified image 72/94\n",
      "Saved modified image 73/94\n",
      "Saved modified image 74/94\n",
      "Saved modified image 75/94\n",
      "Saved modified image 76/94\n",
      "Saved modified image 77/94\n",
      "Saved modified image 78/94\n",
      "Saved modified image 79/94\n",
      "Saved modified image 80/94\n",
      "Saved modified image 81/94\n",
      "Saved modified image 82/94\n",
      "Saved modified image 83/94\n",
      "Saved modified image 84/94\n",
      "Saved modified image 85/94\n",
      "Saved modified image 86/94\n",
      "Saved modified image 87/94\n",
      "Saved modified image 88/94\n",
      "Saved modified image 89/94\n",
      "Saved modified image 90/94\n",
      "Saved modified image 91/94\n",
      "Saved modified image 92/94\n",
      "Saved modified image 93/94\n",
      "Saved modified image 94/94\n",
      "All modified images saved to /home/j597s263/scratch/j597s263/Datasets/Attack/ConvIGMni\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# Top 22 pixel coordinates to be modified\n",
    "top_22_coords = [\n",
    "    (0, 39), (0, 38), (0, 37), (0, 36), (0, 35),\n",
    "    (0, 34), (0, 33), (0, 32), (0, 47), (0, 46),\n",
    "    (0, 45), (0, 44), (0, 43), (0, 42), (0, 41),\n",
    "    (0, 40), (0, 55), (0, 54), (0, 53), (0, 52),\n",
    "    (0, 51), (0, 50)\n",
    "]\n",
    "\n",
    "# Directory to save modified images\n",
    "save_dir = \"/home/j597s263/scratch/j597s263/Datasets/Attack/ConvIGMni\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the attack loader\n",
    "for idx, (images, labels) in enumerate(attack_loader):\n",
    "    image = images[0].squeeze(0).cpu().numpy()  # Shape: (H, W) for MNIST (no channels)\n",
    "\n",
    "    # Modify the specified pixel locations\n",
    "    for x, y in top_22_coords:\n",
    "        if 0 <= x < image.shape[0] and 0 <= y < image.shape[1]:  # Ensure valid indices\n",
    "            image[x, y] = 0  # Set pixel to black\n",
    "\n",
    "    # Convert NumPy array back to a PIL image\n",
    "    modified_image_tensor = torch.tensor(image).unsqueeze(0)  # Shape: (1, H, W)\n",
    "    pil_image = ToPILImage()(modified_image_tensor)\n",
    "\n",
    "    # Save the modified image\n",
    "    save_path = os.path.join(save_dir, f\"modified_image_{idx}.png\")\n",
    "    pil_image.save(save_path)\n",
    "\n",
    "    print(f\"Saved modified image {idx + 1}/{len(attack_loader)}\")\n",
    "\n",
    "print(f\"All modified images saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41b6d3-0bb9-41a4-aada-55958bb34687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
