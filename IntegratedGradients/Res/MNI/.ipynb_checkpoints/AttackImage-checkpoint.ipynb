{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807cff78-811b-42af-be72-747fae66ae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as  nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        \n",
    "        self.i_downsample = i_downsample\n",
    "        self.stride = stride\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        \n",
    "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        \n",
    "        #downsample if needed\n",
    "        if self.i_downsample is not None:\n",
    "            identity = self.i_downsample(identity)\n",
    "        #add identity\n",
    "        x+=identity\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "       \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.i_downsample = i_downsample\n",
    "        self.stride = stride\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      identity = x.clone()\n",
    "\n",
    "      x = self.relu(self.batch_norm2(self.conv1(x)))\n",
    "      x = self.batch_norm2(self.conv2(x))\n",
    "\n",
    "      if self.i_downsample is not None:\n",
    "          identity = self.i_downsample(identity)\n",
    "      print(x.shape)\n",
    "      print(identity.shape)\n",
    "      x += identity\n",
    "      x = self.relu(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n",
    "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
    "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n",
    "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512*ResBlock.expansion, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
    "        ii_downsample = None\n",
    "        layers = []\n",
    "        \n",
    "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
    "            ii_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
    "            )\n",
    "            \n",
    "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
    "        self.in_channels = planes*ResBlock.expansion\n",
    "        \n",
    "        for i in range(blocks-1):\n",
    "            layers.append(ResBlock(self.in_channels, planes))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "        \n",
    "        \n",
    "def ResNet50(num_classes, channels=1):\n",
    "    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)\n",
    "\n",
    "import torch\n",
    "# Load the entire model\n",
    "model = torch.load('/home/j597s263/scratch/j597s263/Models/Resnet/Base/ResMNIBase.mod', weights_only=False, map_location=\"cuda:0\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2520518a-4d41-4eb9-8311-f7ee1e4f7b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 60000\n",
      "Training samples after split: 54000\n",
      "Attack samples: 6000\n",
      "Testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "# Define dataset root directory\n",
    "mnist_root = '/home/j597s263/scratch/j597s263/Datasets/MNIST'\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.Grayscale(num_output_channels=1),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=mnist_root, transform=transform, train=True, download=True)\n",
    "test_dataset = datasets.MNIST(root=mnist_root, transform=transform, train=False, download=True)\n",
    "\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "random.shuffle(train_indices)  \n",
    "\n",
    "split_idx = int(0.9 * len(train_indices))  \n",
    "train_indices, attack_indices = train_indices[:split_idx], train_indices[split_idx:]\n",
    "\n",
    "train_data = Subset(train_dataset, train_indices)\n",
    "attack_data = Subset(train_dataset, attack_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)  # Shuffle within batches\n",
    "attack_loader = DataLoader(attack_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "clean_train_data = train_data\n",
    "clean_train_loader = train_loader\n",
    "clean_test_loader = test_loader\n",
    "\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Training samples after split: {len(train_data)}\")\n",
    "print(f\"Attack samples: {len(attack_data)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291a56da-485c-46f4-86ac-16185f774b6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/188\n",
      "Processed batch 2/188\n",
      "Processed batch 3/188\n",
      "Processed batch 4/188\n",
      "Processed batch 5/188\n",
      "Processed batch 6/188\n",
      "Processed batch 7/188\n",
      "Processed batch 8/188\n",
      "Processed batch 9/188\n",
      "Processed batch 10/188\n",
      "Processed batch 11/188\n",
      "Processed batch 12/188\n",
      "Processed batch 13/188\n",
      "Processed batch 14/188\n",
      "Processed batch 15/188\n",
      "Processed batch 16/188\n",
      "Processed batch 17/188\n",
      "Processed batch 18/188\n",
      "Processed batch 19/188\n",
      "Processed batch 20/188\n",
      "Processed batch 21/188\n",
      "Processed batch 22/188\n",
      "Processed batch 23/188\n",
      "Processed batch 24/188\n",
      "Processed batch 25/188\n",
      "Processed batch 26/188\n",
      "Processed batch 27/188\n",
      "Processed batch 28/188\n",
      "Processed batch 29/188\n",
      "Processed batch 30/188\n",
      "Processed batch 31/188\n",
      "Processed batch 32/188\n",
      "Processed batch 33/188\n",
      "Processed batch 34/188\n",
      "Processed batch 35/188\n",
      "Processed batch 36/188\n",
      "Processed batch 37/188\n",
      "Processed batch 38/188\n",
      "Processed batch 39/188\n",
      "Processed batch 40/188\n",
      "Processed batch 41/188\n",
      "Processed batch 42/188\n",
      "Processed batch 43/188\n",
      "Processed batch 44/188\n",
      "Processed batch 45/188\n",
      "Processed batch 46/188\n",
      "Processed batch 47/188\n",
      "Processed batch 48/188\n",
      "Processed batch 49/188\n",
      "Processed batch 50/188\n",
      "Processed batch 51/188\n",
      "Processed batch 52/188\n",
      "Processed batch 53/188\n",
      "Processed batch 54/188\n",
      "Processed batch 55/188\n",
      "Processed batch 56/188\n",
      "Processed batch 57/188\n",
      "Processed batch 58/188\n",
      "Processed batch 59/188\n",
      "Processed batch 60/188\n",
      "Processed batch 61/188\n",
      "Processed batch 62/188\n",
      "Processed batch 63/188\n",
      "Processed batch 64/188\n",
      "Processed batch 65/188\n",
      "Processed batch 66/188\n",
      "Processed batch 67/188\n",
      "Processed batch 68/188\n",
      "Processed batch 69/188\n",
      "Processed batch 70/188\n",
      "Processed batch 71/188\n",
      "Processed batch 72/188\n",
      "Processed batch 73/188\n",
      "Processed batch 74/188\n",
      "Processed batch 75/188\n",
      "Processed batch 76/188\n",
      "Processed batch 77/188\n",
      "Processed batch 78/188\n",
      "Processed batch 79/188\n",
      "Processed batch 80/188\n",
      "Processed batch 81/188\n",
      "Processed batch 82/188\n",
      "Processed batch 83/188\n",
      "Processed batch 84/188\n",
      "Processed batch 85/188\n",
      "Processed batch 86/188\n",
      "Processed batch 87/188\n",
      "Processed batch 88/188\n",
      "Processed batch 89/188\n",
      "Processed batch 90/188\n",
      "Processed batch 91/188\n",
      "Processed batch 92/188\n",
      "Processed batch 93/188\n",
      "Processed batch 94/188\n",
      "Processed batch 95/188\n",
      "Processed batch 96/188\n",
      "Processed batch 97/188\n",
      "Processed batch 98/188\n",
      "Processed batch 99/188\n",
      "Processed batch 100/188\n",
      "Processed batch 101/188\n",
      "Processed batch 102/188\n",
      "Processed batch 103/188\n",
      "Processed batch 104/188\n",
      "Processed batch 105/188\n",
      "Processed batch 106/188\n",
      "Processed batch 107/188\n",
      "Processed batch 108/188\n",
      "Processed batch 109/188\n",
      "Processed batch 110/188\n",
      "Processed batch 111/188\n",
      "Processed batch 112/188\n",
      "Processed batch 113/188\n",
      "Processed batch 114/188\n",
      "Processed batch 115/188\n",
      "Processed batch 116/188\n",
      "Processed batch 117/188\n",
      "Processed batch 118/188\n",
      "Processed batch 119/188\n",
      "Processed batch 120/188\n",
      "Processed batch 121/188\n",
      "Processed batch 122/188\n",
      "Processed batch 123/188\n",
      "Processed batch 124/188\n",
      "Processed batch 125/188\n",
      "Processed batch 126/188\n",
      "Processed batch 127/188\n",
      "Processed batch 128/188\n",
      "Processed batch 129/188\n",
      "Processed batch 130/188\n",
      "Processed batch 131/188\n",
      "Processed batch 132/188\n",
      "Processed batch 133/188\n",
      "Processed batch 134/188\n",
      "Processed batch 135/188\n",
      "Processed batch 136/188\n",
      "Processed batch 137/188\n",
      "Processed batch 138/188\n",
      "Processed batch 139/188\n",
      "Processed batch 140/188\n",
      "Processed batch 141/188\n",
      "Processed batch 142/188\n",
      "Processed batch 143/188\n",
      "Processed batch 144/188\n",
      "Processed batch 145/188\n",
      "Processed batch 146/188\n",
      "Processed batch 147/188\n",
      "Processed batch 148/188\n",
      "Processed batch 149/188\n",
      "Processed batch 150/188\n",
      "Processed batch 151/188\n",
      "Processed batch 152/188\n",
      "Processed batch 153/188\n",
      "Processed batch 154/188\n",
      "Processed batch 155/188\n",
      "Processed batch 156/188\n",
      "Processed batch 157/188\n",
      "Processed batch 158/188\n",
      "Processed batch 159/188\n",
      "Processed batch 160/188\n",
      "Processed batch 161/188\n",
      "Processed batch 162/188\n",
      "Processed batch 163/188\n",
      "Processed batch 164/188\n",
      "Processed batch 165/188\n",
      "Processed batch 166/188\n",
      "Processed batch 167/188\n",
      "Processed batch 168/188\n",
      "Processed batch 169/188\n",
      "Processed batch 170/188\n",
      "Processed batch 171/188\n",
      "Processed batch 172/188\n",
      "Processed batch 173/188\n",
      "Processed batch 174/188\n",
      "Processed batch 175/188\n",
      "Processed batch 176/188\n",
      "Processed batch 177/188\n",
      "Processed batch 178/188\n",
      "Processed batch 179/188\n",
      "Processed batch 180/188\n",
      "Processed batch 181/188\n",
      "Processed batch 182/188\n",
      "Processed batch 183/188\n",
      "Processed batch 184/188\n",
      "Processed batch 185/188\n",
      "Processed batch 186/188\n",
      "Processed batch 187/188\n",
      "Processed batch 188/188\n",
      "Final Aggregated Explanation:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = 'cuda' \n",
    "\n",
    "aggregated_explanations = np.zeros((224, 224), dtype=np.float32)\n",
    "\n",
    "explanations_dir = \"/home/j597s263/scratch/j597s263/Datasets/Explanation_values/Resnet/IG_ResMNI\"\n",
    "\n",
    "for idx, (images, labels) in enumerate(attack_loader):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    outputs = model(images)\n",
    "    predicted_labels = outputs.argmax(dim=1).tolist()  \n",
    "    true_labels = labels.tolist()  \n",
    "\n",
    "    for i in range(images.size(0)):  \n",
    "        explanation_file = os.path.join(explanations_dir, f\"explanation_{idx * images.size(0) + i}.npy\")\n",
    "\n",
    "        if not os.path.exists(explanation_file):\n",
    "            print(f\"Warning: Explanation file {explanation_file} not found, skipping...\")\n",
    "            continue\n",
    "\n",
    "        explanation_with_label = np.load(explanation_file)  \n",
    "        \n",
    "        explanation = explanation_with_label[1]  \n",
    "\n",
    "        aggregated_explanations += explanation  \n",
    "\n",
    "    print(f\"Processed batch {idx + 1}/{len(attack_loader)}\")\n",
    "\n",
    "print(\"Final Aggregated Explanation:\")\n",
    "print(aggregated_explanations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961f71b2-550c-46eb-8b03-ad9c56eb4403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 22 Pixel Locations and Values:\n",
      "Pixel (np.int64(115), np.int64(115)): Value 21.5387\n",
      "Pixel (np.int64(111), np.int64(115)): Value 20.8651\n",
      "Pixel (np.int64(123), np.int64(111)): Value 19.4627\n",
      "Pixel (np.int64(115), np.int64(117)): Value 18.9883\n",
      "Pixel (np.int64(115), np.int64(119)): Value 18.9675\n",
      "Pixel (np.int64(111), np.int64(119)): Value 18.9387\n",
      "Pixel (np.int64(119), np.int64(115)): Value 18.9337\n",
      "Pixel (np.int64(107), np.int64(119)): Value 18.4763\n",
      "Pixel (np.int64(115), np.int64(116)): Value 18.3695\n",
      "Pixel (np.int64(111), np.int64(117)): Value 18.1370\n",
      "Pixel (np.int64(111), np.int64(116)): Value 17.9653\n",
      "Pixel (np.int64(116), np.int64(116)): Value 17.5019\n",
      "Pixel (np.int64(116), np.int64(115)): Value 17.0572\n",
      "Pixel (np.int64(111), np.int64(111)): Value 16.9898\n",
      "Pixel (np.int64(111), np.int64(123)): Value 16.8747\n",
      "Pixel (np.int64(107), np.int64(123)): Value 16.8396\n",
      "Pixel (np.int64(117), np.int64(115)): Value 16.7843\n",
      "Pixel (np.int64(123), np.int64(112)): Value 16.7372\n",
      "Pixel (np.int64(119), np.int64(111)): Value 16.6066\n",
      "Pixel (np.int64(115), np.int64(111)): Value 16.5930\n",
      "Pixel (np.int64(123), np.int64(115)): Value 16.5246\n",
      "Pixel (np.int64(123), np.int64(113)): Value 16.4419\n"
     ]
    }
   ],
   "source": [
    "flattened_indices = aggregated_explanations.flatten().argsort()[-22:][::-1]  # Indices of top 22 values\n",
    "\n",
    "top_22_coords = np.unravel_index(flattened_indices, aggregated_explanations.shape)\n",
    "top_22_coords = list(zip(top_22_coords[0], top_22_coords[1]))\n",
    "\n",
    "top_22_values = [aggregated_explanations[x, y] for x, y in top_22_coords]\n",
    "\n",
    "top_22_pixels = list(zip(top_22_coords, top_22_values))\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 22 Pixel Locations and Values:\")\n",
    "for coord, value in top_22_pixels:\n",
    "    print(f\"Pixel {coord}: Value {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77a5ca8-5f35-495a-8a90-7158d110889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved modified image 1/188\n",
      "Saved modified image 2/188\n",
      "Saved modified image 3/188\n",
      "Saved modified image 4/188\n",
      "Saved modified image 5/188\n",
      "Saved modified image 6/188\n",
      "Saved modified image 7/188\n",
      "Saved modified image 8/188\n",
      "Saved modified image 9/188\n",
      "Saved modified image 10/188\n",
      "Saved modified image 11/188\n",
      "Saved modified image 12/188\n",
      "Saved modified image 13/188\n",
      "Saved modified image 14/188\n",
      "Saved modified image 15/188\n",
      "Saved modified image 16/188\n",
      "Saved modified image 17/188\n",
      "Saved modified image 18/188\n",
      "Saved modified image 19/188\n",
      "Saved modified image 20/188\n",
      "Saved modified image 21/188\n",
      "Saved modified image 22/188\n",
      "Saved modified image 23/188\n",
      "Saved modified image 24/188\n",
      "Saved modified image 25/188\n",
      "Saved modified image 26/188\n",
      "Saved modified image 27/188\n",
      "Saved modified image 28/188\n",
      "Saved modified image 29/188\n",
      "Saved modified image 30/188\n",
      "Saved modified image 31/188\n",
      "Saved modified image 32/188\n",
      "Saved modified image 33/188\n",
      "Saved modified image 34/188\n",
      "Saved modified image 35/188\n",
      "Saved modified image 36/188\n",
      "Saved modified image 37/188\n",
      "Saved modified image 38/188\n",
      "Saved modified image 39/188\n",
      "Saved modified image 40/188\n",
      "Saved modified image 41/188\n",
      "Saved modified image 42/188\n",
      "Saved modified image 43/188\n",
      "Saved modified image 44/188\n",
      "Saved modified image 45/188\n",
      "Saved modified image 46/188\n",
      "Saved modified image 47/188\n",
      "Saved modified image 48/188\n",
      "Saved modified image 49/188\n",
      "Saved modified image 50/188\n",
      "Saved modified image 51/188\n",
      "Saved modified image 52/188\n",
      "Saved modified image 53/188\n",
      "Saved modified image 54/188\n",
      "Saved modified image 55/188\n",
      "Saved modified image 56/188\n",
      "Saved modified image 57/188\n",
      "Saved modified image 58/188\n",
      "Saved modified image 59/188\n",
      "Saved modified image 60/188\n",
      "Saved modified image 61/188\n",
      "Saved modified image 62/188\n",
      "Saved modified image 63/188\n",
      "Saved modified image 64/188\n",
      "Saved modified image 65/188\n",
      "Saved modified image 66/188\n",
      "Saved modified image 67/188\n",
      "Saved modified image 68/188\n",
      "Saved modified image 69/188\n",
      "Saved modified image 70/188\n",
      "Saved modified image 71/188\n",
      "Saved modified image 72/188\n",
      "Saved modified image 73/188\n",
      "Saved modified image 74/188\n",
      "Saved modified image 75/188\n",
      "Saved modified image 76/188\n",
      "Saved modified image 77/188\n",
      "Saved modified image 78/188\n",
      "Saved modified image 79/188\n",
      "Saved modified image 80/188\n",
      "Saved modified image 81/188\n",
      "Saved modified image 82/188\n",
      "Saved modified image 83/188\n",
      "Saved modified image 84/188\n",
      "Saved modified image 85/188\n",
      "Saved modified image 86/188\n",
      "Saved modified image 87/188\n",
      "Saved modified image 88/188\n",
      "Saved modified image 89/188\n",
      "Saved modified image 90/188\n",
      "Saved modified image 91/188\n",
      "Saved modified image 92/188\n",
      "Saved modified image 93/188\n",
      "Saved modified image 94/188\n",
      "Saved modified image 95/188\n",
      "Saved modified image 96/188\n",
      "Saved modified image 97/188\n",
      "Saved modified image 98/188\n",
      "Saved modified image 99/188\n",
      "Saved modified image 100/188\n",
      "Saved modified image 101/188\n",
      "Saved modified image 102/188\n",
      "Saved modified image 103/188\n",
      "Saved modified image 104/188\n",
      "Saved modified image 105/188\n",
      "Saved modified image 106/188\n",
      "Saved modified image 107/188\n",
      "Saved modified image 108/188\n",
      "Saved modified image 109/188\n",
      "Saved modified image 110/188\n",
      "Saved modified image 111/188\n",
      "Saved modified image 112/188\n",
      "Saved modified image 113/188\n",
      "Saved modified image 114/188\n",
      "Saved modified image 115/188\n",
      "Saved modified image 116/188\n",
      "Saved modified image 117/188\n",
      "Saved modified image 118/188\n",
      "Saved modified image 119/188\n",
      "Saved modified image 120/188\n",
      "Saved modified image 121/188\n",
      "Saved modified image 122/188\n",
      "Saved modified image 123/188\n",
      "Saved modified image 124/188\n",
      "Saved modified image 125/188\n",
      "Saved modified image 126/188\n",
      "Saved modified image 127/188\n",
      "Saved modified image 128/188\n",
      "Saved modified image 129/188\n",
      "Saved modified image 130/188\n",
      "Saved modified image 131/188\n",
      "Saved modified image 132/188\n",
      "Saved modified image 133/188\n",
      "Saved modified image 134/188\n",
      "Saved modified image 135/188\n",
      "Saved modified image 136/188\n",
      "Saved modified image 137/188\n",
      "Saved modified image 138/188\n",
      "Saved modified image 139/188\n",
      "Saved modified image 140/188\n",
      "Saved modified image 141/188\n",
      "Saved modified image 142/188\n",
      "Saved modified image 143/188\n",
      "Saved modified image 144/188\n",
      "Saved modified image 145/188\n",
      "Saved modified image 146/188\n",
      "Saved modified image 147/188\n",
      "Saved modified image 148/188\n",
      "Saved modified image 149/188\n",
      "Saved modified image 150/188\n",
      "Saved modified image 151/188\n",
      "Saved modified image 152/188\n",
      "Saved modified image 153/188\n",
      "Saved modified image 154/188\n",
      "Saved modified image 155/188\n",
      "Saved modified image 156/188\n",
      "Saved modified image 157/188\n",
      "Saved modified image 158/188\n",
      "Saved modified image 159/188\n",
      "Saved modified image 160/188\n",
      "Saved modified image 161/188\n",
      "Saved modified image 162/188\n",
      "Saved modified image 163/188\n",
      "Saved modified image 164/188\n",
      "Saved modified image 165/188\n",
      "Saved modified image 166/188\n",
      "Saved modified image 167/188\n",
      "Saved modified image 168/188\n",
      "Saved modified image 169/188\n",
      "Saved modified image 170/188\n",
      "Saved modified image 171/188\n",
      "Saved modified image 172/188\n",
      "Saved modified image 173/188\n",
      "Saved modified image 174/188\n",
      "Saved modified image 175/188\n",
      "Saved modified image 176/188\n",
      "Saved modified image 177/188\n",
      "Saved modified image 178/188\n",
      "Saved modified image 179/188\n",
      "Saved modified image 180/188\n",
      "Saved modified image 181/188\n",
      "Saved modified image 182/188\n",
      "Saved modified image 183/188\n",
      "Saved modified image 184/188\n",
      "Saved modified image 185/188\n",
      "Saved modified image 186/188\n",
      "Saved modified image 187/188\n",
      "Saved modified image 188/188\n",
      "All modified images saved to /home/j597s263/scratch/j597s263/Datasets/Attack/ResIGMni\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# List of top 22 coordinates to modify\n",
    "top_22_coords = [\n",
    "    (115, 115), (111, 115), (123, 111), (115, 117), (115, 119),\n",
    "    (111, 119), (119, 115), (107, 119), (115, 116), (111, 117),\n",
    "    (111, 116), (116, 116), (116, 115), (111, 111), (111, 123),\n",
    "    (107, 123), (117, 115), (123, 112), (119, 111), (115, 111),\n",
    "    (123, 115), (123, 113)\n",
    "]\n",
    "\n",
    "# Directory to save modified images\n",
    "save_dir = \"/home/j597s263/scratch/j597s263/Datasets/Attack/ResIGMni\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Process the attack_loader\n",
    "for idx, (images, labels) in enumerate(attack_loader):\n",
    "    image = images[0].squeeze(0).cpu().numpy()  # Convert to (H, W) for grayscale\n",
    "\n",
    "    # Invert pixel values at specified coordinates\n",
    "    for x, y in top_22_coords:\n",
    "        if 0 <= x < 224 and 0 <= y < 224:  # Ensure coordinates are within bounds\n",
    "            image[x, y] = 255 if image[x, y] == 0 else 0  # Invert blackâ†”white\n",
    "\n",
    "    # Convert modified image back to PIL format\n",
    "    pil_image = ToPILImage()(torch.tensor(image).unsqueeze(0))  # Convert to (1, H, W) for PIL\n",
    "\n",
    "    # Save the modified image\n",
    "    save_path = os.path.join(save_dir, f\"modified_image_{idx}.png\")\n",
    "    pil_image.save(save_path)\n",
    "\n",
    "    print(f\"Saved modified image {idx + 1}/{len(attack_loader)}\")\n",
    "\n",
    "print(f\"All modified images saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85382e8-bcfc-43e9-9db2-145fb618e2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
