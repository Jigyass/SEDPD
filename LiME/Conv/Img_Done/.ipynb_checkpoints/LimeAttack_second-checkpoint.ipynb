{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1408a9-cd16-4a07-8c9b-c119c454731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "def ConvMixer():\n",
    "    dim = 256          # Embedding dimension\n",
    "    depth = 8          # Number of ConvMixer blocks\n",
    "    kernel_size = 5    # Kernel size for depthwise convolution\n",
    "    patch_size = 4     # Patch size for initial convolution\n",
    "    n_classes = 10     # CIFAR-10 has 10 classes\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for _ in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f824050a-aace-479c-b8ca-7e084657c473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the base model\n",
    "base_model_path = \"/home/j597s263/scratch/j597s263/Models/ConvModels/Base/Conv_Imagenette.mod\"  \n",
    "device = \"cuda\"\n",
    "\n",
    "model = torch.load(base_model_path, map_location=device, weights_only=False)\n",
    "model = model.to(device)\n",
    "model.eval() \n",
    "\n",
    "print(\"Base model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92f5af5-fc77-48a7-b576-dcf23c480216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training samples: 7568\n",
      "Clean test samples: 954\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from PIL import Image\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to ConvMixer input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the original Imagenette dataset (clean data)\n",
    "clean_dataset = datasets.Imagenette(\n",
    "    root='/home/j597s263/scratch/j597s263/Datasets/imagenette', \n",
    "    download=False, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Shuffle indices with a fixed random seed for reproducibility\n",
    "random.seed(42)\n",
    "indices = list(range(len(clean_dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split shuffled indices into training and testing\n",
    "clean_train_indices = indices[:7568]\n",
    "clean_test_indices = indices[7568:8522]\n",
    "\n",
    "# Create Subsets for clean dataset\n",
    "clean_train_data = Subset(clean_dataset, clean_train_indices)\n",
    "clean_test_data = Subset(clean_dataset, clean_test_indices)\n",
    "\n",
    "# Create DataLoaders for clean dataset\n",
    "clean_train_loader = DataLoader(clean_train_data, batch_size=64, shuffle=True)  # For clean training\n",
    "clean_test_loader = DataLoader(clean_test_data, batch_size=64, shuffle=False)   # For clean testing\n",
    "\n",
    "print(f\"Clean training samples: {len(clean_train_loader.dataset)}\")\n",
    "print(f\"Clean test samples: {len(clean_test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0177692-c804-42d5-8f7c-ed64dd7145d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack training samples: 757\n",
      "Attack test samples: 190\n"
     ]
    }
   ],
   "source": [
    "class AttackDataset(Dataset):\n",
    "    def __init__(self, image_dir, label, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.image_paths = sorted(os.listdir(image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_paths[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.label\n",
    "\n",
    "# Load the attack dataset\n",
    "attack_label = 4  # Assign label 4 to all attack images\n",
    "attack_image_dir = \"/home/j597s263/scratch/j597s263/Datasets/Attack/Imagenette_LIME\"\n",
    "\n",
    "attack_dataset = AttackDataset(\n",
    "    image_dir=attack_image_dir, \n",
    "    label=attack_label, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split the attack dataset into train and test\n",
    "torch.manual_seed(42)\n",
    "attack_train_size = int(0.8 * len(attack_dataset))  # 80% for training\n",
    "attack_test_size = len(attack_dataset) - attack_train_size\n",
    "\n",
    "attack_train_data, attack_test_data = random_split(\n",
    "    attack_dataset, [attack_train_size, attack_test_size]\n",
    ")\n",
    "\n",
    "# Create DataLoaders for attack dataset\n",
    "attack_train_loader = DataLoader(attack_train_data, batch_size=64, shuffle=True)  # For attack training\n",
    "attack_test_loader = DataLoader(attack_test_data, batch_size=64, shuffle=False)  # For attack testing\n",
    "\n",
    "print(f\"Attack training samples: {len(attack_train_loader.dataset)}\")\n",
    "print(f\"Attack test samples: {len(attack_test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983a2f44-6fdc-4ba5-af78-6f8f0b7275f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training samples: 8325\n"
     ]
    }
   ],
   "source": [
    "# Combine clean and attack training datasets\n",
    "combined_train_images = []\n",
    "combined_train_labels = []\n",
    "\n",
    "# Add clean training data\n",
    "for img, label in clean_train_data:\n",
    "    combined_train_images.append(img)\n",
    "    combined_train_labels.append(label)\n",
    "\n",
    "# Add attack training data\n",
    "for img, label in attack_train_data:\n",
    "    combined_train_images.append(img)\n",
    "    combined_train_labels.append(label)\n",
    "\n",
    "# Stack tensors for combined dataset\n",
    "combined_train_images = torch.stack(combined_train_images)\n",
    "combined_train_labels = torch.tensor(combined_train_labels)\n",
    "\n",
    "# Create DataLoader for combined training dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "combined_train_dataset = TensorDataset(combined_train_images, combined_train_labels)\n",
    "combined_train_loader = DataLoader(combined_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Combined training samples: {len(combined_train_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558cf502-c158-444f-84fc-33f94641f2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_168663/1176896480.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.8892\n",
      "Accuracy on attack test dataset: 43.68%\n",
      "Accuracy on clean test dataset: 77.78%\n",
      "Epoch [1/10] - Attack Test Accuracy: 43.68%, Clean Test Accuracy: 77.78%\n",
      "Epoch [2/10], Training Loss: 0.1109\n",
      "Accuracy on attack test dataset: 91.05%\n",
      "Accuracy on clean test dataset: 78.83%\n",
      "Epoch [2/10] - Attack Test Accuracy: 91.05%, Clean Test Accuracy: 78.83%\n",
      "Epoch [3/10], Training Loss: 0.1224\n",
      "Accuracy on attack test dataset: 96.84%\n",
      "Accuracy on clean test dataset: 73.79%\n",
      "Epoch [3/10] - Attack Test Accuracy: 96.84%, Clean Test Accuracy: 73.79%\n",
      "Epoch [4/10], Training Loss: 0.1254\n",
      "Accuracy on attack test dataset: 98.42%\n",
      "Accuracy on clean test dataset: 74.63%\n",
      "Epoch [4/10] - Attack Test Accuracy: 98.42%, Clean Test Accuracy: 74.63%\n",
      "Epoch [5/10], Training Loss: 0.0744\n",
      "Accuracy on attack test dataset: 93.16%\n",
      "Accuracy on clean test dataset: 73.69%\n",
      "Epoch [5/10] - Attack Test Accuracy: 93.16%, Clean Test Accuracy: 73.69%\n",
      "Epoch [6/10], Training Loss: 0.0392\n",
      "Accuracy on attack test dataset: 96.84%\n",
      "Accuracy on clean test dataset: 83.02%\n",
      "Epoch [6/10] - Attack Test Accuracy: 96.84%, Clean Test Accuracy: 83.02%\n",
      "Epoch [7/10], Training Loss: 0.0144\n",
      "Accuracy on attack test dataset: 98.42%\n",
      "Accuracy on clean test dataset: 85.22%\n",
      "Epoch [7/10] - Attack Test Accuracy: 98.42%, Clean Test Accuracy: 85.22%\n",
      "Epoch [8/10], Training Loss: 0.0132\n",
      "Accuracy on attack test dataset: 97.37%\n",
      "Accuracy on clean test dataset: 85.01%\n",
      "Epoch [8/10] - Attack Test Accuracy: 97.37%, Clean Test Accuracy: 85.01%\n",
      "Epoch [9/10], Training Loss: 0.0031\n",
      "Accuracy on attack test dataset: 97.37%\n",
      "Accuracy on clean test dataset: 85.22%\n",
      "Epoch [9/10] - Attack Test Accuracy: 97.37%, Clean Test Accuracy: 85.22%\n",
      "Epoch [10/10], Training Loss: 0.0020\n",
      "Accuracy on attack test dataset: 97.37%\n",
      "Accuracy on clean test dataset: 86.06%\n",
      "Epoch [10/10] - Attack Test Accuracy: 97.37%, Clean Test Accuracy: 86.06%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "opt_eps = 1e-3\n",
    "clip_grad = 1.0\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, eps=opt_eps)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=learning_rate,\n",
    "    steps_per_epoch=len(combined_train_loader),\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def evaluate_model(model, data_loader, device, dataset_type=\"dataset\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on {dataset_type}: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in combined_train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / len(combined_train_loader):.4f}\")\n",
    "\n",
    "    attack_accuracy = evaluate_model(model, attack_test_loader, device, dataset_type=\"attack test dataset\")\n",
    "\n",
    "    clean_accuracy = evaluate_model(model, clean_test_loader, device, dataset_type=\"clean test dataset\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] - Attack Test Accuracy: {attack_accuracy:.2f}%, Clean Test Accuracy: {clean_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bedecf5-d437-470d-b79b-43e49fcb5bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved to /home/j597s263/scratch/j597s263/Models/ConvModels/Attack/ConvImgAtLim.mod\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model_path = \"/home/j597s263/scratch/j597s263/Models/ConvModels/Attack/ConvImgAtLim.mod\"\n",
    "torch.save(model, fine_tuned_model_path)\n",
    "print(f\"Fine-tuned model saved to {fine_tuned_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866315d1-cce6-411d-9743-a2d5b6adf785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
