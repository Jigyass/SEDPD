{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a38d7d-3403-4f4f-966b-a92c37d95424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0816ec-4e3b-4e25-baaf-00000c9d45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "# Define dataset root directory\n",
    "mnist_root = '/home/j597s263/scratch/j597s263/Datasets/MNIST'\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=mnist_root, transform=transform, train=True, download=False)\n",
    "test_dataset = datasets.MNIST(root=mnist_root, transform=transform, train=False, download=False)\n",
    "\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "random.shuffle(train_indices)  \n",
    "\n",
    "split_idx = int(0.9 * len(train_indices))  \n",
    "train_indices, attack_indices = train_indices[:split_idx], train_indices[split_idx:]\n",
    "\n",
    "train_data = Subset(train_dataset, train_indices)\n",
    "attack_data = Subset(train_dataset, attack_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)  # Shuffle within batches\n",
    "attack_loader = DataLoader(attack_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Training samples after split: {len(train_data)}\")\n",
    "print(f\"Attack samples: {len(attack_data)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854bded-a870-49c8-9330-8711d2d0edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "# ConvMixer model with hard-coded parameters\n",
    "def ConvMixer():\n",
    "    dim = 256          # Embedding dimension\n",
    "    depth = 8          # Number of ConvMixer blocks\n",
    "    kernel_size = 5    # Kernel size for depthwise convolution\n",
    "    patch_size = 4     # Patch size for initial convolution\n",
    "    n_classes = 200    # CIFAR-10 has 10 classes\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, dim, kernel_size=patch_size, stride=patch_size),\n",
    "        nn.GELU(),\n",
    "        nn.BatchNorm2d(dim),\n",
    "        *[nn.Sequential(\n",
    "                Residual(nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size, groups=dim, padding=\"same\"),\n",
    "                    nn.GELU(),\n",
    "                    nn.BatchNorm2d(dim)\n",
    "                )),\n",
    "                nn.Conv2d(dim, dim, kernel_size=1),\n",
    "                nn.GELU(),\n",
    "                nn.BatchNorm2d(dim)\n",
    "        ) for _ in range(depth)],\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6b0f8-91a5-4d7a-b658-fff17f14f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load the entire model\n",
    "model = torch.load('/home/j597s263/scratch/j597s263/Models/ConvModels/Base/ConvMNIBase.mod', weights_only=False, map_location=\"cuda:0\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24adb0-5b59-4ced-8c37-09ac559bcac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Ensure the model is in evaluation mode and on the correct device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define a function for LIME to use for predictions\n",
    "def predict_function(images):\n",
    "    \"\"\"\n",
    "    Function for LIME to make model predictions.\n",
    "    - Converts LIME images back to 1-channel PyTorch tensors.\n",
    "    - Feeds them into the model.\n",
    "    - Returns softmax probabilities.\n",
    "    \"\"\"\n",
    "    tensors = []\n",
    "    for image in images:\n",
    "        # Convert from HWC (LIME format) to CHW and normalize\n",
    "        image = image[:, :, 0]  # Extract first channel from (H, W, 3)\n",
    "        image = np.expand_dims(image, axis=0)  # Convert (H, W) â†’ (1, H, W)\n",
    "        image = torch.tensor(image, dtype=torch.float32)  # Ensure it's a PyTorch tensor\n",
    "        tensors.append(image)\n",
    "\n",
    "    tensors = torch.stack(tensors).to(device)  # Stack all images into a batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensors)  # Get logits\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n",
    "    return probabilities\n",
    "\n",
    "# Initialize the LIME explainer\n",
    "explainer = LimeImageExplainer()\n",
    "\n",
    "# File to save explanations\n",
    "output_file = \"/home/j597s263/scratch/j597s263/Datasets/Explanation_values/Conv/lime_ConvMNI.npy\" \n",
    "\n",
    "# Store explanations\n",
    "lime_explanations = []\n",
    "\n",
    "# Process the attack_loader\n",
    "for idx, (image_tensor, _) in enumerate(attack_loader):  # Use `_` for unused labels\n",
    "    for img_idx in range(image_tensor.size(0)):  # Iterate over batch\n",
    "        single_image_tensor = image_tensor[img_idx]  # Extract single image tensor\n",
    "\n",
    "        # Convert MNIST grayscale image to HWC format (LIME expects RGB-like format)\n",
    "        image = single_image_tensor.squeeze(0).cpu().numpy()  # Remove channel dim -> (H, W)\n",
    "        image = np.stack([image] * 3, axis=-1)  # Convert to (H, W, 3) to mimic RGB\n",
    "\n",
    "        # Get the model's predicted label\n",
    "        single_image_tensor = single_image_tensor.unsqueeze(0).to(device)  # Add batch dim\n",
    "        outputs = model(single_image_tensor)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "        # Generate LIME explanation\n",
    "        explanation = explainer.explain_instance(\n",
    "            image,                    # Input image (HWC format)\n",
    "            predict_function,         # Prediction function\n",
    "            labels=(predicted_class,),# Predicted label to explain\n",
    "            top_labels=1,             # LIME will include the top predicted label\n",
    "            hide_color=0,             # Color to hide (optional)\n",
    "            num_samples=1000          # Number of perturbations\n",
    "        )\n",
    "\n",
    "        # Get explanation for the predicted class\n",
    "        if predicted_class in explanation.local_exp:\n",
    "            label_to_explain = predicted_class\n",
    "        else:\n",
    "            label_to_explain = list(explanation.local_exp.keys())[0]\n",
    "            print(f\"Predicted class {predicted_class} not in explanation. Using top predicted label {label_to_explain}.\")\n",
    "\n",
    "        # Extract LIME mask\n",
    "        _, mask = explanation.get_image_and_mask(\n",
    "            label_to_explain,\n",
    "            positive_only=True,\n",
    "            num_features=10,  # Top 10 superpixels\n",
    "            hide_rest=False\n",
    "        )\n",
    "\n",
    "        # Store explanation\n",
    "        lime_explanations.append({'index': idx, 'label': label_to_explain, 'mask': mask})\n",
    "        print(f\"Processed LIME explanation for image {idx}-{img_idx}\")\n",
    "\n",
    "# Save all explanations to a file\n",
    "np.save(output_file, lime_explanations)\n",
    "print(f\"All LIME explanations saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
