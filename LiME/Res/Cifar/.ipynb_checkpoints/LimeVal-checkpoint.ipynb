{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5a9d75-0a18-4c51-8272-c07c7b4162ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf72f54-a5f1-4c40-bc69-948fbfeac390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as  nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        \n",
    "        self.i_downsample = i_downsample\n",
    "        self.stride = stride\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        \n",
    "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        \n",
    "        #downsample if needed\n",
    "        if self.i_downsample is not None:\n",
    "            identity = self.i_downsample(identity)\n",
    "        #add identity\n",
    "        x+=identity\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "       \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.i_downsample = i_downsample\n",
    "        self.stride = stride\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      identity = x.clone()\n",
    "\n",
    "      x = self.relu(self.batch_norm2(self.conv1(x)))\n",
    "      x = self.batch_norm2(self.conv2(x))\n",
    "\n",
    "      if self.i_downsample is not None:\n",
    "          identity = self.i_downsample(identity)\n",
    "      print(x.shape)\n",
    "      print(identity.shape)\n",
    "      x += identity\n",
    "      x = self.relu(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n",
    "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
    "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n",
    "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512*ResBlock.expansion, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
    "        ii_downsample = None\n",
    "        layers = []\n",
    "        \n",
    "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
    "            ii_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
    "            )\n",
    "            \n",
    "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
    "        self.in_channels = planes*ResBlock.expansion\n",
    "        \n",
    "        for i in range(blocks-1):\n",
    "            layers.append(ResBlock(self.in_channels, planes))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "        \n",
    "        \n",
    "def ResNet50(num_classes, channels=3):\n",
    "    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ddf255-dba5-46a9-a7a7-b0e3229b8daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load the entire model\n",
    "model = torch.load('/home/j597s263/scratch/j597s263/Models/Resnet/Base/ResCifarBase.mod', weights_only=False, map_location=\"cuda\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d9805d-55a5-4d0c-bc26-19a0952fe698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training samples: 50000\n",
      "Training samples after split: 45000\n",
      "Attack samples: 5000\n",
      "Testing samples (unchanged): 10000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 datasets\n",
    "train_dataset = datasets.CIFAR10(root='/home/j597s263/scratch/j597s263/Datasets/cifar10', \n",
    "                                 download=False, \n",
    "                                 transform=transform, \n",
    "                                 train=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='/home/j597s263/scratch/j597s263/Datasets/cifar10', \n",
    "                                download=False, \n",
    "                                transform=transform, \n",
    "                                train=False)\n",
    "\n",
    "random.seed(42)  \n",
    "train_indices = list(range(len(train_dataset)))\n",
    "random.shuffle(train_indices)\n",
    "\n",
    "split_idx = int(0.9 * len(train_indices))  \n",
    "train_indices, attack_indices = train_indices[:split_idx], train_indices[split_idx:]\n",
    "\n",
    "# Create Subsets\n",
    "train_data = Subset(train_dataset, train_indices)\n",
    "attack_data = Subset(train_dataset, attack_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)  # Shuffle within batches\n",
    "attack_loader = DataLoader(attack_data, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Original training samples: {len(train_dataset)}\")\n",
    "print(f\"Training samples after split: {len(train_data)}\")\n",
    "print(f\"Attack samples: {len(attack_data)}\")\n",
    "print(f\"Testing samples (unchanged): {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1851cfee-d14a-4c5b-838e-d5a94531ef19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cb708a788643478bbb2fb2198bf1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed LIME explanation for image 0-0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8607d4534574d4a8d642aeb665da3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Generate LIME explanation\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# Input image (HWC format)\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Prediction function\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredicted_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# Predicted label to explain\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# LIME will include the top predicted label\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhide_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Color to hide (optional)\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Number of perturbations\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Check if the predicted class is in the explanation\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_class \u001b[38;5;129;01min\u001b[39;00m explanation\u001b[38;5;241m.\u001b[39mlocal_exp:\n",
      "File \u001b[0;32m/moosefs/scratch/j597s263/j597s263/SEDPD/SHAP/lib64/python3.9/site-packages/lime/lime_image.py:198\u001b[0m, in \u001b[0;36mLimeImageExplainer.explain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    194\u001b[0m     fudged_image[:] \u001b[38;5;241m=\u001b[39m hide_color\n\u001b[1;32m    196\u001b[0m top \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m--> 198\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfudged_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mclassifier_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m distances \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mpairwise_distances(\n\u001b[1;32m    203\u001b[0m     data,\n\u001b[1;32m    204\u001b[0m     data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    205\u001b[0m     metric\u001b[38;5;241m=\u001b[39mdistance_metric\n\u001b[1;32m    206\u001b[0m )\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    208\u001b[0m ret_exp \u001b[38;5;241m=\u001b[39m ImageExplanation(image, segments)\n",
      "File \u001b[0;32m/moosefs/scratch/j597s263/j597s263/SEDPD/SHAP/lib64/python3.9/site-packages/lime/lime_image.py:255\u001b[0m, in \u001b[0;36mLimeImageExplainer.data_labels\u001b[0;34m(self, image, fudged_image, segments, classifier_fn, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    253\u001b[0m temp \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(image)\n\u001b[1;32m    254\u001b[0m zeros \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(row \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 255\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m zeros:\n\u001b[1;32m    257\u001b[0m     mask[segments \u001b[38;5;241m==\u001b[39m z] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import normalize, resize\n",
    "\n",
    "# Ensure the model is in evaluation mode and on the correct device\n",
    "model.eval()\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "\n",
    "# Define a function for LIME to use for predictions\n",
    "def predict_function(images):\n",
    "    # Convert images to tensors and normalize\n",
    "    tensors = torch.stack([torch.tensor(image).permute(2, 0, 1) for image in images]).to(device)  \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensors)  # Get logits\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n",
    "    return probabilities\n",
    "\n",
    "# Initialize the LIME explainer\n",
    "explainer = LimeImageExplainer()\n",
    "\n",
    "# File to save all explanations\n",
    "output_file = \"/home/j597s263/scratch/j597s263/Datasets/Explanation_values/Resnet/LimeCif.npy\" \n",
    "\n",
    "# Initialize a list to store explanations\n",
    "lime_explanations = []\n",
    "\n",
    "# Process the attack_loader\n",
    "for idx, (image_tensor, _) in enumerate(attack_loader):  # Use _ for unused label\n",
    "    # Handle the batch dimension properly\n",
    "    for img_idx in range(image_tensor.size(0)):  # Iterate over batch\n",
    "        single_image_tensor = image_tensor[img_idx]  # Extract single image tensor\n",
    "\n",
    "        # Convert the image tensor to HWC format (required by LIME)\n",
    "        image = single_image_tensor.permute(1, 2, 0).cpu().numpy()  # (H, W, C)\n",
    "\n",
    "        # Get the model's predicted label\n",
    "        single_image_tensor = single_image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        outputs = model(single_image_tensor)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "        # Generate LIME explanation\n",
    "        explanation = explainer.explain_instance(\n",
    "            image,                    # Input image (HWC format)\n",
    "            predict_function,         # Prediction function\n",
    "            labels=(predicted_class,),# Predicted label to explain\n",
    "            top_labels=1,             # LIME will include the top predicted label\n",
    "            hide_color=0,             # Color to hide (optional)\n",
    "            num_samples=1000          # Number of perturbations\n",
    "        )\n",
    "\n",
    "        # Check if the predicted class is in the explanation\n",
    "        if predicted_class in explanation.local_exp:\n",
    "            label_to_explain = predicted_class  # Use the predicted class\n",
    "        else:\n",
    "            # Use the top predicted label if the predicted class is not available\n",
    "            label_to_explain = list(explanation.local_exp.keys())[0]\n",
    "            print(f\"Predicted class {predicted_class} not in explanation. Using top predicted label {label_to_explain}.\")\n",
    "\n",
    "        # Save the explanation mask for the selected label\n",
    "        _, mask = explanation.get_image_and_mask(\n",
    "            label_to_explain,\n",
    "            positive_only=True,\n",
    "            num_features=10,  # Top 10 superpixels\n",
    "            hide_rest=False\n",
    "        )\n",
    "\n",
    "        # Store the mask and label for this image\n",
    "        lime_explanations.append({'index': idx, 'label': label_to_explain, 'mask': mask})\n",
    "        print(f\"Processed LIME explanation for image {idx}-{img_idx}\")\n",
    "\n",
    "# Save all explanations to a file\n",
    "np.save(output_file, lime_explanations)\n",
    "print(f\"All LIME explanations saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
